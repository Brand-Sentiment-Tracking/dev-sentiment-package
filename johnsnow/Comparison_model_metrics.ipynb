{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brand-Sentiment-Tracking/dev-sentiment-package/blob/main/johnsnow/Comparison_model_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oYgZXG4gDt1"
      },
      "source": [
        "# **Class for Sentiment Analysis for News Articles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9xk47mgkcB"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pDySD2IHU9di"
      },
      "outputs": [],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "! pip install -q pyspark==3.1.2 spark-nlp\n",
        "\n",
        "# Install Spark NLP Display lib\n",
        "! pip install --upgrade -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bnOW8Mt7VH97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effd3992-c43f-4da7-9902-ea916062aa16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  3.4.3\n",
            "Apache Spark version:  3.1.2\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "import pyspark.sql.functions as F\n",
        "from tabulate import tabulate\n",
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp_display import NerVisualizer\n",
        "\n",
        "# spark = sparknlp.start(gpu=False)\n",
        "spark = sparknlp.start(gpu=True)\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Brand Identification Class"
      ],
      "metadata": {
        "id": "rmq_-RXkbB1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_brand(row_list):\n",
        "    if not row_list: # If the list is empty\n",
        "        return \"None\"\n",
        "\n",
        "    else:\n",
        "        # Create a pandas df with entity names and types\n",
        "        data = [[row.result, row.metadata['entity']] for row in row_list]\n",
        "        df_pd = pd.DataFrame(data, columns = ['Entity', 'Type'])\n",
        "      \n",
        "        # Filter only ORGs\n",
        "        df_pd = df_pd[df_pd[\"Type\"] == \"ORG\"]\n",
        "\n",
        "        # Rank the ORGs by frequencies\n",
        "        ranked_df = df_pd[\"Entity\"].value_counts() # a Pandas Series object\n",
        "            \n",
        "        # If no ORG identified in headline, return None\n",
        "        if len(ranked_df.index) == 0:\n",
        "           return \"None\"\n",
        "\n",
        "        # If only one ORG appears in headline, return it\n",
        "        elif len(ranked_df.index) == 1:\n",
        "           return ranked_df.index[0]\n",
        "\n",
        "        # If one ORG appear more than the others, return that one \n",
        "        elif ranked_df[0] > ranked_df[1]:\n",
        "            return ranked_df.index[0] \n",
        "\n",
        "        else: # If multiple ORGs appear the same time, return randomly (TO BE MODIFIED)\n",
        "            return random.choice([ranked_df.index[0], ranked_df.index[1]])\n",
        "            # TO DO: break even - Wikidata for article body #"
      ],
      "metadata": {
        "id": "OdQPrJe86RQz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BrandIdentification:\n",
        "    def __init__(self, MODEL_NAME):\n",
        "        self.MODEL_NAME = MODEL_NAME\n",
        "\n",
        "        # Define Spark NLP pipeline \n",
        "        documentAssembler = DocumentAssembler() \\\n",
        "            .setInputCol('text') \\\n",
        "            .setOutputCol('document')\n",
        "\n",
        "        tokenizer = Tokenizer() \\\n",
        "            .setInputCols(['document']) \\\n",
        "            .setOutputCol('token')\n",
        "\n",
        "        # ner_dl and onto_100 model are trained with glove_100d, so the embeddings in the pipeline should match\n",
        "        if (self.MODEL_NAME == \"ner_dl\") or (self.MODEL_NAME == \"onto_100\"):\n",
        "            embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
        "                .setInputCols([\"document\", 'token']) \\\n",
        "                .setOutputCol(\"embeddings\")\n",
        "\n",
        "        # Bert model uses Bert embeddings\n",
        "        elif self.MODEL_NAME == \"ner_dl_bert\":\n",
        "            embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
        "                .setInputCols(['document', 'token']) \\\n",
        "                .setOutputCol('embeddings')\n",
        "\n",
        "        ner_model = NerDLModel.pretrained(MODEL_NAME, 'en') \\\n",
        "            .setInputCols(['document', 'token', 'embeddings']) \\\n",
        "            .setOutputCol('ner')\n",
        "\n",
        "        ner_converter = NerConverter() \\\n",
        "            .setInputCols(['document', 'token', 'ner']) \\\n",
        "            .setOutputCol('ner_chunk')\n",
        "\n",
        "        nlp_pipeline = Pipeline(stages=[\n",
        "            documentAssembler, \n",
        "            tokenizer,\n",
        "            embeddings,\n",
        "            ner_model,\n",
        "            ner_converter\n",
        "        ])\n",
        "        \n",
        "        # Create the pipeline model\n",
        "        empty_df = spark.createDataFrame([['']]).toDF('text') # An empty df with column name \"text\"\n",
        "        self.pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "\n",
        "\n",
        "    def predict_brand(self, text): # text could be a pandas dataframe or a Spark dataframe (with a column \"text\"), a list of strings\n",
        "        # Run the pipeline for the text\n",
        "        if isinstance(text, pd.DataFrame): \n",
        "            text_df = spark.createDataFrame(text) # If input a pandas dataframe\n",
        "        elif isinstance(text, list): \n",
        "            text_df = spark.createDataFrame(pd.DataFrame({'text': text})) # If input a list of strings\n",
        "        elif isinstance(text, str): \n",
        "            text_df = spark.createDataFrame(pd.DataFrame({'text': text}, index=[0])) # If input a single string\n",
        "        else: text_df = text\n",
        "\n",
        "        df_spark = self.pipeline_model.transform(text_df) \n",
        "\n",
        "        # Improve speed of identification using Spark User-defined function\n",
        "        pred_brand = F.udf(lambda z: get_brand(z), StringType()) # Output a string\n",
        "        # spark.udf.register(\"pred_brand\", pred_brand)\n",
        "\n",
        "        df_spark_combined = df_spark.withColumn('Predicted_brand', pred_brand('ner_chunk'))\n",
        "        df_spark_combined = df_spark_combined.select(\"text\", \"Predicted_brand\")\n",
        "        # df_spark_combined.show(100)\n",
        "        \n",
        "        # Remove all rows with no brands detected\n",
        "        df_spark_final=df_spark_combined.filter(df_spark_combined.Predicted_brand != 'None')\n",
        "        df_spark_final.show(100)\n",
        "\n",
        "        return df_spark_final\n"
      ],
      "metadata": {
        "id": "sYVn0VZzc-Yf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Senitment Identification Class"
      ],
      "metadata": {
        "id": "-5m65UosNdY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentIdentification:\n",
        "\n",
        "    def __init__(self, MODEL_NAME):\n",
        "        \"\"\"Creates a class for sentiment identication using specified model.\n",
        "\n",
        "        Args:\n",
        "          MODEL_NAME: Name of the Spark NLP pretrained pipeline.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the pipeline instance\n",
        "        self.MODEL_NAME = MODEL_NAME\n",
        "\n",
        "        # Define pipeline to train\n",
        "        if self.MODEL_NAME == \"untrained_pipeline\":\n",
        "            document = DocumentAssembler()\\\n",
        "                .setInputCol(\"text\")\\\n",
        "                .setOutputCol(\"document\")\n",
        "\n",
        "            use = UniversalSentenceEncoder.pretrained() \\\n",
        "            .setInputCols([\"document\"])\\\n",
        "            .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "            # the classes/labels/categories are in category column\n",
        "            sentimentdl = SentimentDLApproach()\\\n",
        "              .setInputCols([\"sentence_embeddings\"])\\\n",
        "              .setOutputCol(\"class\")\\\n",
        "              .setLabelColumn(\"label\")\\\n",
        "              .setMaxEpochs(5)\\\n",
        "              .setEnableOutputLogs(True)\n",
        "\n",
        "            pipeline = Pipeline(\n",
        "                stages = [\n",
        "                    document,\n",
        "                    use,\n",
        "                    sentimentdl\n",
        "                ])\n",
        "\n",
        "            self.pipeline_model = pipeline.fit(spark.createDataFrame([['']]).toDF(\"text\"))\n",
        "\n",
        "          # Create a custom pipline if requested\n",
        "        if self.MODEL_NAME == \"custom_pipeline\": # https://nlp.johnsnowlabs.com/2021/11/03/bert_sequence_classifier_finbert_en.html\n",
        "            document_assembler = DocumentAssembler() \\\n",
        "                .setInputCol('text') \\\n",
        "                .setOutputCol('document')\n",
        "\n",
        "            tokenizer = Tokenizer() \\\n",
        "                .setInputCols(['document']) \\\n",
        "                .setOutputCol('token')\n",
        "\n",
        "            sequenceClassifier = BertForSequenceClassification \\\n",
        "                  .pretrained('bert_base_sequence_classifier_imdb', 'en') \\\n",
        "                  .setInputCols(['token', 'document']) \\\n",
        "                  .setOutputCol('class') \\\n",
        "                  .setCaseSensitive(True) \\\n",
        "                  .setMaxSentenceLength(512)\n",
        "            # bert_sequence_classifier_finbert\n",
        "            pipeline = Pipeline(stages=[\n",
        "                document_assembler,\n",
        "                tokenizer,\n",
        "                sequenceClassifier\n",
        "            ])\n",
        "\n",
        "            self.pipeline_model = pipeline.fit(spark.createDataFrame([['']]).toDF(\"text\"))\n",
        "\n",
        "        else:\n",
        "            self.pipeline_model = PretrainedPipeline(self.MODEL_NAME, lang = 'en')\n",
        "\n",
        "\n",
        "    def predict_string_list(self, string_list):\n",
        "        \"\"\"Predicts sentiment of the input list of strings.\n",
        "\n",
        "        Args:\n",
        "          string_list: List of strings to classify.\n",
        "        \"\"\"\n",
        " \n",
        "        # Annotate input text using pretrained model\n",
        "\n",
        "        if self.MODEL_NAME == \"custom_pipeline\":\n",
        "            pipeline_annotator = LightPipeline(self.pipeline_model) # Convert the pipeline to an annotator\n",
        "        else:\n",
        "            pipeline_annotator = self.pipeline_model\n",
        "\n",
        "        annotations =  pipeline_annotator.annotate(string_list)\n",
        "\n",
        "        if self.MODEL_NAME  == \"custom_pipeline\" or  self.MODEL_NAME == \"classifierdl_bertwiki_finance_sentiment_pipeline\":\n",
        "            return [annotation['class'][0] for annotation in annotations] # Return the sentiment list of strings if using bert pipeline\n",
        "        else:\n",
        "            return [annotation['sentiment'][0] for annotation in annotations] # if using imdb model\n",
        "\n",
        "    def predict_dataframe(self, df):\n",
        "        \"\"\"Annotates the input dataframe with the classification results.\n",
        "\n",
        "        Args:\n",
        "          df : Pandas or Spark dataframe to classify (must contain a \"text\" column)\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(df, pd.DataFrame):\n",
        "            # Convert to spark dataframe for faster prediction\n",
        "            df_spark = spark.createDataFrame(df) \n",
        "        else:\n",
        "            df_spark = df\n",
        "\n",
        "        # Annotate dataframe with classification results\n",
        "        df_spark = self.pipeline_model.transform(df_spark)\n",
        "\n",
        "        # Visusalize schemas\n",
        "        # df_spark.printSchema()\n",
        "        # print(df_spark.select(explode(col(\"sentiment.result\"))).collect()[10])\n",
        "        # print(df_spark.select(explode(col(\"sentiment.metadata\"))).collect()[0])\n",
        "\n",
        "        #Extract sentiment score\n",
        "        if self.MODEL_NAME == \"custom_pipeline\":\n",
        "          df_spark_scores = df_spark.select(explode(col(\"class.metadata\")).alias(\"metadata\")).select(col(\"metadata\")[\"Some(positive)\"].alias(\"positive\"),\n",
        "                                                                                            col(\"metadata\")[\"Some(neutral)\"].alias(\"neutral\"),\n",
        "                                                                                            col(\"metadata\")[\"Some(negative)\"].alias(\"negative\"))\n",
        "        elif self.MODEL_NAME == \"classifierdl_bertwiki_finance_sentiment_pipeline\":\n",
        "          df_spark_scores = df_spark.select(explode(col(\"class.metadata\")).alias(\"metadata\")).select(col(\"metadata\")[\"positive\"].alias(\"positive\"),\n",
        "                                                                                            col(\"metadata\")[\"neutral\"].alias(\"neutral\"),\n",
        "                                                                                            col(\"metadata\")[\"negative\"].alias(\"negative\"))\n",
        "          \n",
        "        elif self.MODEL_NAME == \"analyze_sentimentdl_use_twitter\":\n",
        "          df_spark_scores = df_spark.select(explode(col(\"sentiment.metadata\")).alias(\"metadata\")).select(col(\"metadata\")[\"positive\"].alias(\"positive\"),\n",
        "                                                                                                  col(\"metadata\")[\"negative\"].alias(\"negative\"))\n",
        "        else:\n",
        "          df_spark_scores = df_spark.select(explode(col(\"sentiment.metadata\")).alias(\"metadata\")).select(col(\"metadata\")[\"pos\"].alias(\"positive\"),\n",
        "                                                                                            col(\"metadata\")[\"neg\"].alias(\"negative\"))\n",
        "\n",
        "        \n",
        "        # Extract only target and label columns\n",
        "        if self.MODEL_NAME == \"custom_pipeline\" or self.MODEL_NAME == \"classifierdl_bertwiki_finance_sentiment_pipeline\":\n",
        "              df_spark = df_spark.select(\"text\", \"True_Sentiment\", \"class.result\")\n",
        "        else:\n",
        "              df_spark = df_spark.select(\"text\", \"True_Sentiment\", \"sentiment.result\")\n",
        "\n",
        "        # Rename to result column to Predicted Sentiment\n",
        "        df_spark = df_spark.withColumnRenamed(\"result\", \"Predicted_Sentiment\")\n",
        "\n",
        "        # Convert sentiment from a list to a string\n",
        "        df_spark = df_spark.withColumn(\"Predicted_Sentiment\", array_join(\"Predicted_Sentiment\", \"\"))\n",
        "\n",
        "        # Join the predictions dataframe to the scores dataframe\n",
        "        # Add temporary column index to join\n",
        "        w = Window.orderBy(monotonically_increasing_id())\n",
        "        df_spark_with_index =  df_spark.withColumn(\"columnindex\", row_number().over(w))\n",
        "        df_spark_scores_with_index =  df_spark_scores.withColumn(\"columnindex\", row_number().over(w))\n",
        "\n",
        "        # Join the predictions and the scores in one dataframe\n",
        "        df_spark_with_index = df_spark_with_index.join(df_spark_scores_with_index,\n",
        "                                df_spark_with_index.columnindex == df_spark_scores_with_index.columnindex,\n",
        "                                'inner').drop(df_spark_scores_with_index.columnindex)\n",
        "\n",
        "        # Remove the index column\n",
        "        df_spark_combined = df_spark_with_index.drop(df_spark_with_index.columnindex)\n",
        "\n",
        "        # Convert to pandas dataframe for postprocessing (https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32)\n",
        "        df_pandas_postprocessed = df_spark_combined.toPandas()\n",
        "\n",
        "        return df_pandas_postprocessed\n",
        "\n",
        "\n",
        "    def compute_accuracy(self, df_pandas_postprocessed):\n",
        "        \"\"\"Computes accuracy by comparing labels of input dataframe.\n",
        "\n",
        "        Args:\n",
        "          df_pandas_postprocessed: pandas dataframe containing \"True_Sentiment\" and \"Predicted_Sentiment\" columns\n",
        "        \"\"\"\n",
        "    \n",
        "        from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "        # Compute the accuracy\n",
        "        accuracy = accuracy_score(df_pandas_postprocessed[\"True_Sentiment\"], df_pandas_postprocessed[\"Predicted_Sentiment\"])\n",
        "        accuracy *= 100\n",
        "        classification_report = classification_report(df_pandas_postprocessed[\"True_Sentiment\"], df_pandas_postprocessed[\"Predicted_Sentiment\"])\n",
        "\n",
        "        # Alternatively if the input is a postprocessed spark dataframe\n",
        "        # Compute accuracy by comparing each true label with predicted label\n",
        "        # accuracy = df_spark.filter(df_spark.Predicted_Sentiment == df_spark.True_Sentiment).count()/ num_sentences\n",
        "\n",
        "        return accuracy, classification_report"
      ],
      "metadata": {
        "id": "62AefsLpNiTB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jz3QlAbR6wBr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the accuracy of sentiment using the Financial News Headline Dataset"
      ],
      "metadata": {
        "id": "5GVcdPcr3Ldk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment"
      ],
      "metadata": {
        "id": "zqhY6YdzDOxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Sentiment Test data Financial Phrase Bank"
      ],
      "metadata": {
        "id": "LA3mYD2kK2V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Kaggle data to Pandas dataframe and preprocess\n",
        "import time\n",
        "\n",
        "sentiment_url = 'https://raw.githubusercontent.com/Brand-Sentiment-Tracking/python-package/main/data/sentiment_test_data.csv' # Financial Phrase Bank\n",
        "\n",
        "# Store data in a Pandas Dataframe\n",
        "df_pandas = pd.read_csv(sentiment_url, header=None)\n",
        "\n",
        "\n",
        "# Change column names (pipelines require a \"text\" column to predict)\n",
        "df_pandas.columns = ['True_Sentiment', 'text']\n",
        "\n",
        "# Shuffle the DataFrame rows\n",
        "# df_pandas = df_pandas.sample(frac = 1)\n",
        "\n",
        "# Make dataset smaller for faster runtime\n",
        "num_sentences = 4846 # Total is 4846\n",
        "total_num_sentences = df_pandas.shape[0]\n",
        "df_pandas.drop(df_pandas.index[num_sentences:total_num_sentences], inplace=True)\n",
        "\n",
        "print(df_pandas.shape)\n",
        "\n",
        "# Create a preprocessed spark dataframe\n",
        "from pyspark import SparkFiles\n",
        "spark.sparkContext.addFile(sentiment_url)\n",
        "\n",
        "# Read raw dataframe\n",
        "df_spark = spark.read.csv(\"file://\"+SparkFiles.get(\"sentiment_test_data.csv\"))\n",
        "\n",
        "# Rename columns\n",
        "df_spark = df_spark.withColumnRenamed(\"_c0\", \"True_Sentiment\").withColumnRenamed(\"_c1\", \"text\")\n",
        "df_spark = df_spark.limit(num_sentences)"
      ],
      "metadata": {
        "id": "nwLHDZohOPJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020c6a17-2a3a-4205-ddbc-1af044199296"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4846, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Political News Dataset"
      ],
      "metadata": {
        "id": "m7oPkWGjtcJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Kaggle data to Pandas dataframe and preprocess\n",
        "import pandas as pd\n",
        "\n",
        "sentiment_url_2 = 'https://raw.githubusercontent.com/Brand-Sentiment-Tracking/python-package/main/data/SEN_en_R.csv' # SEN data\n",
        "\n",
        "# Store data in a Pandas Dataframe\n",
        "df_pandas_2 = pd.read_csv(sentiment_url_2, header=None, skiprows=1)\n",
        "\n",
        "df_pandas_2.columns = ['idx', 'text', 'Entity', 'True_Sentiment']\n",
        "\n",
        "# Modify labels to match positive, neutral, negative\n",
        "df_pandas_2 = df_pandas_2.replace({'True_Sentiment': {'pos' : 'positive', 'neg' : 'negative', 'neutr' : 'neutral'}})\n",
        "\n",
        "\n",
        "# Make dataset smaller for faster runtime\n",
        "num_sentences = 1271 # Total is 4846\n",
        "total_num_sentences = df_pandas_2.shape[0]\n",
        "df_pandas_2.drop(df_pandas_2.index[num_sentences:total_num_sentences], inplace=True)\n",
        "\n",
        "# Filter unk labels\n",
        "df_pandas_2 = df_pandas_2[df_pandas_2['True_Sentiment']!='unk']\n",
        "\n",
        "\n",
        "print(df_pandas_2.shape)\n",
        "display(df_pandas_2)\n",
        "\n",
        "\n",
        "# Create a preprocessed spark dataframe\n",
        "from pyspark import SparkFiles\n",
        "spark.sparkContext.addFile(sentiment_url_2)\n",
        "\n",
        "# Read raw dataframe\n",
        "df_spark = spark.read.option(\"header\",\"true\").csv(\"file://\"+SparkFiles.get(\"SEN_en_R.csv\"))\n",
        "\n",
        "# Rename columns\n",
        "df_spark = df_spark.withColumnRenamed(\"_c0\", \"idx\").withColumnRenamed(\"_c1\", \"text\") \\\n",
        "                                                  .withColumnRenamed(\"_c2\", 'Entity') \\\n",
        "                                                  .withColumnRenamed(\"_c3\", 'True_Sentiment')\n",
        "df_spark = df_spark.limit(num_sentences)\n",
        "\n",
        "df_spark.show()\n"
      ],
      "metadata": {
        "id": "YTmQSp28qAK-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "outputId": "ec50a02d-59d4-4575-be2e-72b7f4d45044"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1237, 4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       idx                                               text   Entity  \\\n",
              "0        0  Boris Johnson Joins Trump in Redefining Conser...    Trump   \n",
              "1        1       Trump Praises Controversial Hungarian Leader    Trump   \n",
              "2        2  Stung by Trump’s Criticisms of Russian Gas Dea...    Trump   \n",
              "3        3  What ‘America First’ Means Under Trump Is Comi...    Trump   \n",
              "4        4    Trump Calls for NATO Expansion Into Middle East    Trump   \n",
              "...    ...                                                ...      ...   \n",
              "1266  1266  Ivanka Trump was friends with former British s...    Trump   \n",
              "1267  1267  How could 63 million people be wrong? The GOP ...    Trump   \n",
              "1268  1268  What Trump claimed about the Russia probe — an...    Trump   \n",
              "1269  1269  If officials objecting to Trump’s candidacy wa...    Trump   \n",
              "1270  1270  The Technology 202: Bernie Sanders just made I...  Sanders   \n",
              "\n",
              "     True_Sentiment  \n",
              "0           neutral  \n",
              "1          negative  \n",
              "2           neutral  \n",
              "3           neutral  \n",
              "4          positive  \n",
              "...             ...  \n",
              "1266       negative  \n",
              "1267       negative  \n",
              "1268        neutral  \n",
              "1269        neutral  \n",
              "1270        neutral  \n",
              "\n",
              "[1237 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff02e6e5-1615-43c7-87b4-b3fbc5bcb17b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>Entity</th>\n",
              "      <th>True_Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Boris Johnson Joins Trump in Redefining Conser...</td>\n",
              "      <td>Trump</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Trump Praises Controversial Hungarian Leader</td>\n",
              "      <td>Trump</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Stung by Trump’s Criticisms of Russian Gas Dea...</td>\n",
              "      <td>Trump</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>What ‘America First’ Means Under Trump Is Comi...</td>\n",
              "      <td>Trump</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Trump Calls for NATO Expansion Into Middle East</td>\n",
              "      <td>Trump</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>1266</td>\n",
              "      <td>Ivanka Trump was friends with former British s...</td>\n",
              "      <td>Trump</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>1267</td>\n",
              "      <td>How could 63 million people be wrong? The GOP ...</td>\n",
              "      <td>Trump</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1268</th>\n",
              "      <td>1268</td>\n",
              "      <td>What Trump claimed about the Russia probe — an...</td>\n",
              "      <td>Trump</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1269</th>\n",
              "      <td>1269</td>\n",
              "      <td>If officials objecting to Trump’s candidacy wa...</td>\n",
              "      <td>Trump</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1270</th>\n",
              "      <td>1270</td>\n",
              "      <td>The Technology 202: Bernie Sanders just made I...</td>\n",
              "      <td>Sanders</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1237 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff02e6e5-1615-43c7-87b4-b3fbc5bcb17b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff02e6e5-1615-43c7-87b4-b3fbc5bcb17b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff02e6e5-1615-43c7-87b4-b3fbc5bcb17b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+------+--------------+\n",
            "|idx|            headline|entity|majority_label|\n",
            "+---+--------------------+------+--------------+\n",
            "|  0|Boris Johnson Joi...| Trump|         neutr|\n",
            "|  1|Trump Praises Con...| Trump|           neg|\n",
            "|  2|Stung by Trump’s ...| Trump|         neutr|\n",
            "|  3|What ‘America Fir...| Trump|         neutr|\n",
            "|  4|Trump Calls for N...| Trump|           pos|\n",
            "|  5|Trump Set to Meet...| Trump|         neutr|\n",
            "|  6|Trump Clashes Wit...| Trump|         neutr|\n",
            "|  7|Donald Trump Won’...| Trump|           pos|\n",
            "|  8|Trump to Attend N...| Trump|         neutr|\n",
            "|  9|Trump-Erdogan Rap...| Trump|         neutr|\n",
            "| 10|Trump Says Erdoga...| Trump|         neutr|\n",
            "| 11|Ukrainian Preside...| Trump|         neutr|\n",
            "| 12|In Months Before ...| Trump|           neg|\n",
            "| 13|Ukraine to Review...| Biden|         neutr|\n",
            "| 14|For Trump, Long O...| Trump|         neutr|\n",
            "| 15|Taliban Negotiato...| Trump|           neg|\n",
            "| 16|Trump Administrat...| Trump|           neg|\n",
            "| 17|U.S. Sanctions Ti...| Putin|         neutr|\n",
            "| 18|New York State Su...| Trump|         neutr|\n",
            "| 19|Trump’s NASA Budg...| Trump|           pos|\n",
            "+---+--------------------+------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classify using Pandas Dataframe as input"
      ],
      "metadata": {
        "id": "PJc83LMsgJqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_join\n",
        "from pyspark.sql.functions import col, explode, expr, greatest\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "\n",
        "# Create identifier\n",
        "# identifier_pretrained = SentimentIdentification(MODEL_NAME = \"classifierdl_bertwiki_finance_sentiment_pipeline\")\n",
        "# identifier_pretrained = SentimentIdentification(MODEL_NAME = \"custom_pipeline\")\n",
        "# identifier_pretrained = SentimentIdentification(MODEL_NAME = \"untrained_pipeline\")\n",
        "# identifier_pretrained = SentimentIdentification(MODEL_NAME = \"analyze_sentimentdl_glove_imdb\") \n",
        "# identifier_pretrained = SentimentIdentification(MODEL_NAME = \"analyze_sentimentdl_use_imdb\")\n",
        "# identifier_pretrained = SentimentIdentification(MODEL_NAME = \"analyze_sentiment\")\n",
        "identifier_pretrained = SentimentIdentification(MODEL_NAME = \"analyze_sentimentdl_use_twitter\")\n",
        "\n",
        "# Change dataframe\n",
        "# df_pandas = df_pandas_2\n",
        "\n",
        "start = time.time()\n",
        "df_pandas_postprocessed = identifier_pretrained.predict_dataframe(df_pandas)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{end-start} seconds elapsed to classify {num_sentences} sentences.\")\n",
        "\n",
        "# Modify predicted labels to match with true labels\n",
        "df_pandas_postprocessed = df_pandas_postprocessed.replace({'Predicted_Sentiment': {'pos' : 'positive', 'neg' : 'negative'}})\n",
        "\n",
        "display(df_pandas_postprocessed)\n",
        "\n",
        "# Print accuracy metrics\n",
        "accuracy, report = identifier_pretrained.compute_accuracy(df_pandas_postprocessed)\n",
        "print(accuracy)\n",
        "print(report)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy report for custom pipeline on financial headlines:\n",
        "\n",
        "# 88.44407758976476\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.82      0.96      0.88       604\n",
        "#      neutral       0.94      0.87      0.90      2879\n",
        "#     positive       0.81      0.88      0.85      1363\n",
        "\n",
        "#     accuracy                           0.88      4846\n",
        "#    macro avg       0.86      0.90      0.88      4846\n",
        "# weighted avg       0.89      0.88      0.89      4846\n",
        "\n",
        "\n",
        "# Accuracy report for classifierdl_bertwiki_finance_sentiment_pipeline pipeline on financial headlines:\n",
        "\n",
        "# 90.09492364836979\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.92      0.86      0.89       604\n",
        "#      neutral       0.91      0.94      0.93      2879\n",
        "#     positive       0.87      0.83      0.85      1363\n",
        "\n",
        "#     accuracy                           0.90      4846\n",
        "#    macro avg       0.90      0.88      0.89      4846\n",
        "# weighted avg       0.90      0.90      0.90      4846\n",
        "\n",
        "\n",
        "# Accuracy report for twitter model on financial headlines:\n",
        "# 28.951712752785802\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.25      0.75      0.38       604\n",
        "#      neutral       0.54      0.03      0.05      2879\n",
        "#     positive       0.30      0.64      0.41      1363\n",
        "\n",
        "#     accuracy                           0.29      4846\n",
        "#    macro avg       0.37      0.47      0.28      4846\n",
        "# weighted avg       0.44      0.29      0.19      4846\n",
        "\n",
        "# Accuracy report for bert_base_sequence_classifier_imbd pipeline on political headlines:\n",
        "\n",
        "\n",
        "\n",
        "# Accuracy report for custom pipeline on political headlines:\n",
        "\n",
        "# 49.15117219078415\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.56      0.19      0.29       449\n",
        "#      neutral       0.48      0.87      0.62       571\n",
        "#     positive       0.42      0.11      0.18       217\n",
        "\n",
        "#     accuracy                           0.49      1237\n",
        "#    macro avg       0.49      0.39      0.36      1237\n",
        "# weighted avg       0.50      0.49      0.42      1237\n",
        "\n",
        "# Accuracy report for classifierdl_bertwiki_finance_sentiment_pipeline pipeline on political headlines:\n",
        "\n",
        "# 47.696038803556995\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.52      0.12      0.20       449\n",
        "#      neutral       0.48      0.86      0.62       571\n",
        "#     positive       0.41      0.19      0.26       217\n",
        "\n",
        "#     accuracy                           0.48      1237\n",
        "#    macro avg       0.47      0.39      0.36      1237\n",
        "# weighted avg       0.48      0.48      0.40      1237\n",
        "\n",
        "# Accuracy report for analyze_sentimentdl_glove_imdb on political headlines\n",
        "\n",
        "# 37.10590137429264\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.46      0.66      0.54       449\n",
        "#      neutral       0.52      0.03      0.05       571\n",
        "#     positive       0.26      0.67      0.38       217\n",
        "\n",
        "#     accuracy                           0.37      1237\n",
        "#    macro avg       0.41      0.45      0.32      1237\n",
        "# weighted avg       0.45      0.37      0.29      1237\n",
        "\n",
        "# Accuracy report for analyze_sentimentdl_use_imdb on political headlines\n",
        "\n",
        "# 29.426030719482622\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.44      0.44      0.44       449\n",
        "#      neutral       0.62      0.02      0.03       571\n",
        "#     positive       0.20      0.72      0.32       217\n",
        "\n",
        "#     accuracy                           0.29      1237\n",
        "#    macro avg       0.42      0.39      0.26      1237\n",
        "# weighted avg       0.48      0.29      0.23      1237\n",
        "\n",
        "# Accuracy report for analyze_sentiment\n",
        "\n",
        "# 28.37510105092967\n",
        "#                                   precision    recall  f1-score   support\n",
        "\n",
        "#                               na       0.00      0.00      0.00         0\n",
        "#                         negative       0.38      0.59      0.46       449\n",
        "#                 negativenegative       0.00      0.00      0.00         0\n",
        "#         negativenegativenegative       0.00      0.00      0.00         0\n",
        "#                 negativepositive       0.00      0.00      0.00         0\n",
        "#         negativepositivenegative       0.00      0.00      0.00         0\n",
        "# negativepositivenegativenegative       0.00      0.00      0.00         0\n",
        "#         negativepositivepositive       0.00      0.00      0.00         0\n",
        "#                          neutral       0.00      0.00      0.00       571\n",
        "#                         positive       0.21      0.40      0.27       217\n",
        "#                       positivena       0.00      0.00      0.00         0\n",
        "#                 positivenegative       0.00      0.00      0.00         0\n",
        "#                 positivepositive       0.00      0.00      0.00         0\n",
        "#         positivepositivepositive       0.00      0.00      0.00         0\n",
        "\n",
        "#                         accuracy                           0.28      1237\n",
        "#                        macro avg       0.04      0.07      0.05      1237\n",
        "#                     weighted avg       0.17      0.28      0.21      1237\n",
        "\n",
        "\n",
        "# Accuracy report for analyze_sentimentdl_use_twitter\n",
        "\n",
        "# 35.89329021827001\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.43      0.67      0.53       449\n",
        "#      neutral       0.49      0.04      0.07       571\n",
        "#     positive       0.24      0.55      0.34       217\n",
        "\n",
        "#     accuracy                           0.36      1237\n",
        "#    macro avg       0.39      0.42      0.31      1237\n",
        "# weighted avg       0.42      0.36      0.28      1237"
      ],
      "metadata": {
        "id": "plG0dY_MZu5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "679f6a5a-a896-4d48-9839-825bf1bd4622"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analyze_sentimentdl_use_twitter download started this may take some time.\n",
            "Approx size to download 935.1 MB\n",
            "[OK!]\n",
            "74.93413925170898 seconds elapsed to classify 1271 sentences.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                   text True_Sentiment  \\\n",
              "0     According to Gran , the company has no plans t...        neutral   \n",
              "1     Technopolis plans to develop in stages an area...        neutral   \n",
              "2     The international electronic industry company ...       negative   \n",
              "3     With the new production plant the company woul...       positive   \n",
              "4     According to the company 's updated strategy f...       positive   \n",
              "...                                                 ...            ...   \n",
              "4841  LONDON MarketWatch -- Share prices ended lower...       negative   \n",
              "4842  Rinkuskiai 's beer sales fell by 6.5 per cent ...        neutral   \n",
              "4843  Operating profit fell to EUR 35.4 mn from EUR ...       negative   \n",
              "4844  Net sales of the Paper segment decreased to EU...       negative   \n",
              "4845  Sales in Finland decreased by 10.5 % in Januar...       negative   \n",
              "\n",
              "     Predicted_Sentiment       positive      negative  \n",
              "0               negative    0.049131747    0.95086825  \n",
              "1               positive      0.9986187  0.0013813168  \n",
              "2               negative  9.0321495E-15           1.0  \n",
              "3               positive      0.9408187   0.059181254  \n",
              "4               positive            1.0  5.2342273E-9  \n",
              "...                  ...            ...           ...  \n",
              "4841            negative            0.0           1.0  \n",
              "4842            negative     0.18757151    0.81242853  \n",
              "4843            negative  1.7573632E-18           1.0  \n",
              "4844            negative      0.1898243     0.8101757  \n",
              "4845            negative  2.0608166E-23           1.0  \n",
              "\n",
              "[4846 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00013ca0-dfa8-4703-9c54-8d88d07d134a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>True_Sentiment</th>\n",
              "      <th>Predicted_Sentiment</th>\n",
              "      <th>positive</th>\n",
              "      <th>negative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.049131747</td>\n",
              "      <td>0.95086825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.9986187</td>\n",
              "      <td>0.0013813168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>9.0321495E-15</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.9408187</td>\n",
              "      <td>0.059181254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.2342273E-9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4841</th>\n",
              "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4842</th>\n",
              "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.18757151</td>\n",
              "      <td>0.81242853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4843</th>\n",
              "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.7573632E-18</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4844</th>\n",
              "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>0.1898243</td>\n",
              "      <td>0.8101757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
              "      <td>negative</td>\n",
              "      <td>negative</td>\n",
              "      <td>2.0608166E-23</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4846 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00013ca0-dfa8-4703-9c54-8d88d07d134a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00013ca0-dfa8-4703-9c54-8d88d07d134a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00013ca0-dfa8-4703-9c54-8d88d07d134a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28.951712752785802\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.25      0.75      0.38       604\n",
            "     neutral       0.54      0.03      0.05      2879\n",
            "    positive       0.30      0.64      0.41      1363\n",
            "\n",
            "    accuracy                           0.29      4846\n",
            "   macro avg       0.37      0.47      0.28      4846\n",
            "weighted avg       0.44      0.29      0.19      4846\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict using Spark Dataframe Input"
      ],
      "metadata": {
        "id": "EDnSYA1jzkNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create identifier\n",
        "# identifier_pretrained = SentimentIdentification(MODEL_NAME = \"classifierdl_bertwiki_finance_sentiment_pipeline\")\n",
        "identifier_pretrained = SentimentIdentification(MODEL_NAME = \"custom_pipeline\")\n",
        "\n",
        "start = time.time()\n",
        "# df_pandas_postprocessed = identifier_pretrained.predict_sp_dataframe(df_spark)\n",
        "df_pandas_postprocessed = identifier_pretrained.predict_dataframe(df_spark)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{end-start} seconds elapsed to classify {num_sentences} sentences.\")\n",
        "\n",
        "display(df_pandas_postprocessed)"
      ],
      "metadata": {
        "id": "atfuAY97ztbD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "0caaf9dc-8419-4f08-df1b-890b84576374"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_sequence_classifier_imdb download started this may take some time.\n",
            "Approximate size to download 387.6 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-4d2c97f98255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# df_pandas_postprocessed = identifier_pretrained.predict_sp_dataframe(df_spark)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_pandas_postprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentifier_pretrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9895cf31bb45>\u001b[0m in \u001b[0;36mpredict_dataframe\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Annotate dataframe with classification results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mdf_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_spark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Visusalize schemas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: Dataset does not have any 'text' column"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify the sentiment in each sentence one by one"
      ],
      "metadata": {
        "id": "0jQZPrKIOP3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the identifier object\n",
        "# identifier = SentimentIdentification(MODEL_NAME = \"custom_pipeline\") # 90.2% accuracy on 500 sentences 89.8% on 1000 sentences\n",
        "# identifier = SentimentIdentification(MODEL_NAME =  \"classifierdl_bertwiki_finance_sentiment_pipeline\") # Alternative pretrained pipeline 90.0% accuracy on 500 sentences\n",
        "# identifier = SentimentIdentification(MODEL_NAME =  \"classifierdl_bertwiki_finance_sentiment_pipeline\") # Alternative pretrained pipeline 90.0% accuracy on 500 sentences\n",
        "# identifier = SentimentIdentification(MODEL_NAME = \"analyze_sentimentdl_glove_imdb\") \n",
        "# identifier = SentimentIdentification(MODEL_NAME = \"analyze_sentimentdl_use_imdb\")\n",
        "# identifier = SentimentIdentification(MODEL_NAME = \"analyze_sentiment\")\n",
        "identifier = SentimentIdentification(MODEL_NAME = \"analyze_sentimentdl_use_twitter\")\n",
        "\n",
        "df_pandas = df_pandas_2\n",
        "\n",
        "# sentence = ['Trump Clashes With Macron on NATO, Trade and Islamic State']\n",
        "# print(identifier.predict_string_list(sentence))\n",
        "\n",
        "preds = []\n",
        "target = []\n",
        "ignored_idxs = []\n",
        "sentiment_to_ignore = \"neutral\" # e.g. neutral\n",
        "\n",
        "# Measure how long it takes\n",
        "start = time.time()\n",
        "\n",
        "# Collect predicted sentiment for each headline - take three minutes to run\n",
        "for idx, hl in enumerate(df_pandas['text']):\n",
        "\n",
        "    # Only append the sentiment if it is not the sentiment to ignore (e.g. neutral)\n",
        "    target_sentiment = df_pandas[\"True_Sentiment\"][df_pandas.index[idx]]\n",
        "\n",
        "    if target_sentiment != sentiment_to_ignore:\n",
        "      preds.append(identifier.predict_string_list([hl])[0])\n",
        "    else:\n",
        "      ignored_idxs.append(idx)\n",
        "\n",
        "    # Print progress\n",
        "    if idx % 50 == 0:\n",
        "      print(f\"Classification {100*idx/num_sentences}% done.\")\n",
        "\n",
        "# Remove all ignored entries from dataset\n",
        "df_pandas_postprocessed = df_pandas.drop(df_pandas.index[ignored_idxs], inplace=False)\n",
        "\n",
        "df_pandas_postprocessed['Predicted_Sentiment'] = preds\n",
        "\n",
        "# Measure how long it takes\n",
        "end = time.time()\n",
        "print(f\"{end-start} seconds elapsed to classify {num_sentences} sentences.\")\n",
        "\n",
        "# Modify predicted labels to match with true labels\n",
        "df_pandas_postprocessed = df_pandas_postprocessed.replace({'Predicted_Sentiment': {'pos' : 'positive', 'neg' : 'negative'}})\n",
        "\n",
        "\n",
        "accuracy, report = identifier.compute_accuracy(df_pandas_postprocessed)\n",
        "print(accuracy)\n",
        "print(report)\n",
        "\n",
        "# Accuracy report for analyze_sentimentdl_glove_imdb\n",
        "\n",
        "# 61.0574478901881\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.44      0.80      0.57       604\n",
        "#      neutral       0.00      0.00      0.00         0\n",
        "#     positive       0.87      0.52      0.66      1363\n",
        "\n",
        "#     accuracy                           0.61      1967\n",
        "#    macro avg       0.44      0.44      0.41      1967\n",
        "# weighted avg       0.74      0.61      0.63      1967\n",
        "\n",
        "\n",
        "# Accuracy report for analyze_sentimentdl_use_imdb\n",
        "\n",
        "# 71.42857142857143\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.61      0.22      0.32       604\n",
        "#      neutral       0.00      0.00      0.00         0\n",
        "#     positive       0.73      0.93      0.82      1363\n",
        "\n",
        "#     accuracy                           0.71      1967\n",
        "#    macro avg       0.45      0.38      0.38      1967\n",
        "# weighted avg       0.69      0.71      0.67      1967\n",
        "\n",
        "\n",
        "# Accuracy report for analyze_sentiment\n",
        "\n",
        "# 44.941535332994405\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.31      0.63      0.41       604\n",
        "#     positive       0.69      0.37      0.48      1363\n",
        "\n",
        "#     accuracy                           0.45      1967\n",
        "#    macro avg       0.50      0.50      0.45      1967\n",
        "# weighted avg       0.57      0.45      0.46      1967\n",
        "\n",
        "# Accuracy report for twitter pipeline on financial data:\n",
        "# 67.56481952211489\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.51      0.75      0.61       604\n",
        "#      neutral       0.00      0.00      0.00         0\n",
        "#     positive       0.87      0.64      0.74      1363\n",
        "\n",
        "#     accuracy                           0.68      1967\n",
        "#    macro avg       0.46      0.46      0.45      1967\n",
        "# weighted avg       0.76      0.68      0.70      1967\n",
        "\n",
        "\n",
        "# Accuracy report for twitter pipeline on political data: \n",
        "\n",
        "# 63.36336336336337\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.77      0.67      0.72       449\n",
        "#      neutral       0.00      0.00      0.00         0\n",
        "#     positive       0.48      0.55      0.52       217\n",
        "\n",
        "#     accuracy                           0.63       666\n",
        "#    macro avg       0.42      0.41      0.41       666\n",
        "# weighted avg       0.67      0.63      0.65       666\n",
        "\n",
        "# Accuracy report for analyze_sentimentdl_glove_imdb on political data: \n",
        "\n",
        "# 66.66666666666666\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.81      0.66      0.73       449\n",
        "#      neutral       0.00      0.00      0.00         0\n",
        "#     positive       0.51      0.67      0.58       217\n",
        "\n",
        "#     accuracy                           0.67       666\n",
        "#    macro avg       0.44      0.45      0.44       666\n",
        "# weighted avg       0.71      0.67      0.68       666\n",
        "\n",
        "# Accuracy report for analyze_sentimentdl_use_imdb on political data: \n",
        "\n",
        "# 53.153153153153156\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#     negative       0.76      0.44      0.56       449\n",
        "#      neutral       0.00      0.00      0.00         0\n",
        "#     positive       0.39      0.72      0.50       217\n",
        "\n",
        "#     accuracy                           0.53       666\n",
        "#    macro avg       0.38      0.39      0.35       666\n",
        "# weighted avg       0.64      0.53      0.54       666"
      ],
      "metadata": {
        "id": "5jFhoWw54zMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193f0de6-60e6-441c-ee89-3f230d427768"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analyze_sentimentdl_use_twitter download started this may take some time.\n",
            "Approx size to download 935.1 MB\n",
            "[OK!]\n",
            "Classification 0.0% done.\n",
            "Classification 3.933910306845004% done.\n",
            "Classification 7.867820613690008% done.\n",
            "Classification 11.801730920535011% done.\n",
            "Classification 15.735641227380016% done.\n",
            "Classification 19.66955153422502% done.\n",
            "Classification 23.603461841070022% done.\n",
            "Classification 27.53737214791503% done.\n",
            "Classification 31.471282454760033% done.\n",
            "Classification 35.405192761605036% done.\n",
            "Classification 39.33910306845004% done.\n",
            "Classification 43.273013375295044% done.\n",
            "Classification 47.206923682140044% done.\n",
            "Classification 51.14083398898505% done.\n",
            "Classification 55.07474429583006% done.\n",
            "Classification 59.00865460267506% done.\n",
            "Classification 62.942564909520065% done.\n",
            "Classification 66.87647521636507% done.\n",
            "Classification 70.81038552321007% done.\n",
            "Classification 74.74429583005508% done.\n",
            "Classification 78.67820613690007% done.\n",
            "Classification 82.61211644374508% done.\n",
            "Classification 86.54602675059009% done.\n",
            "Classification 90.4799370574351% done.\n",
            "Classification 94.41384736428009% done.\n",
            "15.112251043319702 seconds elapsed to classify 1271 sentences.\n",
            "63.36336336336337\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.67      0.72       449\n",
            "     neutral       0.00      0.00      0.00         0\n",
            "    positive       0.48      0.55      0.52       217\n",
            "\n",
            "    accuracy                           0.63       666\n",
            "   macro avg       0.42      0.41      0.41       666\n",
            "weighted avg       0.67      0.63      0.65       666\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pandas_postprocessed"
      ],
      "metadata": {
        "id": "R2vymeSy4qyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measure the Accuracy"
      ],
      "metadata": {
        "id": "OugF9Z-6t0PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = df_pandas_postprocessed['True_Sentiment'].to_numpy()\n",
        "y_pred = df_pandas_postprocessed['Predicted_Sentiment'].to_numpy()\n",
        "\n",
        "\n",
        "print(f\"The accuracy is {100* sum(y_true==y_pred)/len(y_true)}%. \\n\")\n",
        "\n",
        "target_names = ['positive', 'neutral', 'negative']\n",
        "\n",
        "# Compute classification metrics - poor accuracy\n",
        "print(classification_report(y_true, y_pred))#, target_names=target_names))"
      ],
      "metadata": {
        "id": "JiLV0Glo_Kzj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Comparison_model_metrics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}