{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brand-Sentiment-Tracking/python-package/blob/main/johnsnow/Charlize_SNOW_LABS_PIPELINES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oYgZXG4gDt1"
      },
      "source": [
        "# **Class for Sentiment Analysis for News Articles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9xk47mgkcB"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pDySD2IHU9di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70f71b91-ffb3-4813-d729-21a103ca2449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 212.4 MB 60 kB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 63.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 23.4 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 66 kB 6.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "! pip install -q pyspark==3.1.2 spark-nlp\n",
        "\n",
        "# Install Spark NLP Display lib\n",
        "! pip install --upgrade -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnOW8Mt7VH97",
        "outputId": "77ce3b29-b1a8-4c87-f592-bd3fab9eb36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  3.4.1\n",
            "Apache Spark version:  3.1.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "\n",
        "import sparknlp\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml import Pipeline\n",
        "from tabulate import tabulate\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from sparknlp_display import NerVisualizer\n",
        "\n",
        "\n",
        "spark = sparknlp.start(gpu=False)\n",
        "# spark = sparknlp.start(gpu=True)\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a News Article"
      ],
      "metadata": {
        "id": "BDijGosdPGzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article = [ # two strings - headline & article body\n",
        "\"\"\"Google sued in US over 'deceptive' location tracking\"\"\", # headline\n",
        "\"\"\"Google is being sued in the US over accusations it deceived people about how to control location tracking.\n",
        "\n",
        "The legal action refers to a widely reported 2018 revelation turning off one location-tracking setting in its apps was insufficient to fully disable the feature.\n",
        "\n",
        "It accuses Google of using so-called dark patterns, marketing techniques that deliberately confuse.\n",
        "\n",
        "Google said the claims were inaccurate and outdated.\n",
        "\n",
        "'Unfair practices'\n",
        "The legal action was filed in the District of Columbia. Similar ones were also filed in Texas, Indiana and Washington state.\n",
        "\n",
        "It refers to an Associated Press revelation turning off Location History when using Google Maps or Search was insufficient - as a separate setting, Web and App Activity, continued to log location and other personal data.\n",
        "\n",
        "The study, with researchers at Princeton University, found up to two billion Android and Apple devices could be affected.\n",
        "\n",
        "\"Google has relied on, and continues to rely on, deceptive and unfair practices that make it difficult for users to decline location tracking or to evaluate the data collection and processing to which they are purportedly consenting,\" the legal action alleges.\n",
        "\n",
        "'Robust controls'\n",
        "Google told BBC News the case was based \"on inaccurate claims and outdated assertions about our settings\".\n",
        "\n",
        "A representative added: \"We have always built privacy features into our products and provided robust controls for location data.\n",
        "\n",
        "\"We will vigorously defend ourselves and set the record straight.\"\n",
        "\n",
        "Visual misdirection\n",
        "The legal action claims Google's policies contained other \"misleading, ambiguous and incomplete descriptions... but guarantee that consumers will not understand when their location is collected and retained by Google or for what purposes\".\n",
        "\n",
        "It refers to dark patterns, design choices that alter users' decision-making for the designer's benefit - such as, complicated navigation menus, visual misdirection, confusing wording and repeated nudging towards a particular outcome.\n",
        "\n",
        "Data regulators are increasingly focusing on these practices.\n",
        "\n",
        "Google faces a raft of other legal actions in the US, including:\n",
        "\n",
        "In May 2020, Arizona filed a legal action over the same issue\n",
        "In December 2020, multiple US states sued over the price and process of advertising auctions\n",
        "In October 2020, the US Justice Department alleged Google had a monopoly over search and search advertising\"\"\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "kzD5yHCBTfKL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Brand Identification Class"
      ],
      "metadata": {
        "id": "rmq_-RXkbB1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BrandIdentification:\n",
        "    def __init__(self, MODEL_NAME):\n",
        "        self.MODEL_NAME = MODEL_NAME\n",
        "\n",
        "        # Define Spark NLP pipeline \n",
        "        documentAssembler = DocumentAssembler() \\\n",
        "            .setInputCol('text') \\\n",
        "            .setOutputCol('document')\n",
        "\n",
        "        tokenizer = Tokenizer() \\\n",
        "            .setInputCols(['document']) \\\n",
        "            .setOutputCol('token')\n",
        "\n",
        "        # ner_dl and onto_100 model are trained with glove_100d, so the embeddings in the pipeline should match\n",
        "        if (self.MODEL_NAME == \"ner_dl\") or (self.MODEL_NAME == \"onto_100\"):\n",
        "            embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
        "                .setInputCols([\"document\", 'token']) \\\n",
        "                .setOutputCol(\"embeddings\")\n",
        "\n",
        "        # Bert model uses Bert embeddings\n",
        "        elif self.MODEL_NAME == \"ner_dl_bert\":\n",
        "            embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
        "                .setInputCols(['document', 'token']) \\\n",
        "                .setOutputCol('embeddings')\n",
        "\n",
        "        ner_model = NerDLModel.pretrained(MODEL_NAME, 'en') \\\n",
        "            .setInputCols(['document', 'token', 'embeddings']) \\\n",
        "            .setOutputCol('ner')\n",
        "\n",
        "        ner_converter = NerConverter() \\\n",
        "            .setInputCols(['document', 'token', 'ner']) \\\n",
        "            .setOutputCol('ner_chunk')\n",
        "\n",
        "        nlp_pipeline = Pipeline(stages=[\n",
        "            documentAssembler, \n",
        "            tokenizer,\n",
        "            embeddings,\n",
        "            ner_model,\n",
        "            ner_converter\n",
        "        ])\n",
        "        \n",
        "        # Create the pipeline model\n",
        "        empty_df = spark.createDataFrame([['']]).toDF('text') # An empty df with column name \"text\"\n",
        "        self.pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "\n",
        "\n",
        "    def create_ranked_table(self, text): # text could be a pandas dataframe with a column \"text\" or a list of strings or a single string\n",
        "        # Run the pipeline for the text\n",
        "        if isinstance(text, pd.DataFrame): text_df = spark.createDataFrame(text) # If input a pandas dataframe\n",
        "        elif isinstance(text, str): text_df = spark.createDataFrame(pd.DataFrame({'text': text}, index=[0])) # If input a single string\n",
        "        else: text_df = spark.createDataFrame(pd.DataFrame({'text': text})) # If input a list of strings\n",
        "        result = self.pipeline_model.transform(text_df)\n",
        "        \n",
        "        # Create a table with entity names and types\n",
        "        df = result.select(F.explode(F.arrays_zip('document.result', 'ner_chunk.result',\"ner_chunk.metadata\")).alias(\"cols\")).select(\\\n",
        "        F.expr(\"cols['1']\").alias(\"chunk\"),\n",
        "        F.expr(\"cols['2'].entity\").alias('result'))\n",
        "        \n",
        "        # Filter only ORGs\n",
        "        df = df.filter(df.result == 'ORG')\n",
        "\n",
        "        # Rank the ORGs by frequencies\n",
        "        ranked_df = df.groupBy(df.chunk).count().orderBy('count', ascending=False)\n",
        "\n",
        "        return df, ranked_df\n",
        "\n",
        "\n",
        "    def predict_by_headline(self, headline): # headline can be a pandas df, a list of string or a single string\n",
        "        _, ranked_df_hl = self.create_ranked_table(headline)\n",
        "        # ranked_df_hl.show(100, truncate=False)\n",
        "        \n",
        "        # If no ORG identified in headline, return None\n",
        "        if ranked_df_hl.count() == 0:\n",
        "            return \"None\" # a string\n",
        "        # If only one ORG appears in headline, return it\n",
        "        elif ranked_df_hl.count() == 1:\n",
        "            return ranked_df_hl.first()[0]\n",
        "        # If one ORG appear more than the others, return that one \n",
        "        elif ranked_df_hl.first()[1] > ranked_df_hl.collect()[1][1]:\n",
        "            return ranked_df_hl.first()[0] \n",
        "        else: # If multiple ORGs appear the same time, leave decision to article body (TO BE MODIFIED)\n",
        "            # return \"None\"\n",
        "            return random.choice([ranked_df_hl.first()[0], ranked_df_hl.collect()[1][0]])\n",
        "\n",
        "\n",
        "    def predict(self, headline, body): # body can be a pandas df, a list of string or a single string\n",
        "        _, ranked_df = self.create_ranked_table(body)\n",
        "\n",
        "        # Return the ORG with highest freq (at least >= 2)\n",
        "        if ranked_df.first()[1] >= 2: \n",
        "            return ranked_df.first()[0] \n",
        "        else:\n",
        "            return \"None\" #  a string\n",
        "        # TO DO: break even - Wikidata#"
      ],
      "metadata": {
        "id": "5JQeWnV1a8Fh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Sentiment Identification Class"
      ],
      "metadata": {
        "id": "-5m65UosNdY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentIdentification:\n",
        "\n",
        "    def __init__(self, MODEL_NAME):\n",
        "        \"\"\"Creates a class for sentiment identication using specified model.\n",
        "\n",
        "        Args:\n",
        "          MODEL_NAME: Name of the Spark NLP pretrained pipeline.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create the pipeline instance\n",
        "        self.MODEL_NAME = MODEL_NAME\n",
        "\n",
        "        if self.MODEL_NAME == \"custom_pipeline\": # https://nlp.johnsnowlabs.com/2021/11/03/bert_sequence_classifier_finbert_en.html\n",
        "          document_assembler = DocumentAssembler() \\\n",
        "              .setInputCol('text') \\\n",
        "              .setOutputCol('document')\n",
        "\n",
        "          tokenizer = Tokenizer() \\\n",
        "              .setInputCols(['document']) \\\n",
        "              .setOutputCol('token')\n",
        "\n",
        "          sequenceClassifier = BertForSequenceClassification \\\n",
        "                .pretrained('bert_sequence_classifier_finbert', 'en') \\\n",
        "                .setInputCols(['token', 'document']) \\\n",
        "                .setOutputCol('class') \\\n",
        "                .setCaseSensitive(True) \\\n",
        "                .setMaxSentenceLength(512)\n",
        "\n",
        "          pipeline = Pipeline(stages=[\n",
        "              document_assembler,\n",
        "              tokenizer,\n",
        "              sequenceClassifier\n",
        "          ])\n",
        "\n",
        "          self.pipeline_model = LightPipeline(pipeline.fit(spark.createDataFrame([['']]).toDF(\"text\")))\n",
        "\n",
        "        else:\n",
        "          self.pipeline_model = PretrainedPipeline(self.MODEL_NAME, lang = 'en')\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predicts sentiment of the input string..\n",
        "\n",
        "        Args:\n",
        "          text: String to classify.\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "\n",
        "        # Annotate input text using pretrained model\n",
        "        annotations =  self.pipeline_model.annotate(self.text)\n",
        "\n",
        "        # Depending on the chosen pipeline the outputs will be slightly different\n",
        "        if self.MODEL_NAME == \"analyze_sentimentdl_glove_imdb\":\n",
        "          # print(f\"{annotations['sentiment']} {annotations['document']}\")\n",
        "\n",
        "          if isinstance(self.text, list):\n",
        "            return [annotation['sentiment'][0] for annotation in annotations] # Return the sentiment list of strings\n",
        "          else:\n",
        "            return annotations['sentiment'][0] # Return the sentiment string\n",
        "\n",
        "        else:\n",
        "          # print(f\"{annotations['class']} {annotations['document']}\")\n",
        "\n",
        "          if isinstance(self.text, list):\n",
        "            return [annotation['class'][0] for annotation in annotations] # Return the sentiment list of strings\n",
        "          else:\n",
        "            return annotations['class'][0] # Return the sentiment string"
      ],
      "metadata": {
        "id": "62AefsLpNiTB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify Brand in news article\n"
      ],
      "metadata": {
        "id": "MwBY37mRbKjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"ner_dl_bert\" # MODEL_NAME = \"onto_100\"\n",
        "\n",
        "brand_identifier = BrandIdentification(MODEL_NAME)\n",
        "headline, body = article\n",
        "\n",
        "brand_by_headline = brand_identifier.predict_by_headline(headline)\n",
        "print(brand_by_headline)\n",
        "\n",
        "# Only use article body if no brand identified in the headline\n",
        "if brand_by_headline == None:\n",
        "    brand = brand_identifier.predict(body)\n",
        "    print(brand)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUgFEXqubJwk",
        "outputId": "cca7dca6-6d02-44f5-fd05-dcc6d4ae259f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n",
            "ner_dl_bert download started this may take some time.\n",
            "Approximate size to download 15.4 MB\n",
            "[OK!]\n",
            "Google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classify article using chosen pipeline"
      ],
      "metadata": {
        "id": "HoTrh-sEUeRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# identifier = SentimentIdentification(MODEL_NAME =  \"analyze_sentimentdl_glove_imdb\")\n",
        "# identifier = SentimentIdentification(MODEL_NAME =  \"classifierdl_bertwiki_finance_sentiment_pipeline\")\n",
        "identifier = SentimentIdentification(MODEL_NAME = \"custom_pipeline\") # Uses https://nlp.johnsnowlabs.com/2021/11/03/bert_sequence_classifier_finbert_en.html\n",
        "\n",
        "# Predict by headline\n",
        "headline = article[0]\n",
        "identifier.predict(headline)\n",
        "\n",
        "# Predict by body\n",
        "body = article[1]\n",
        "identifier.predict(body)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RMwzvEYnPKEQ",
        "outputId": "ce6735cd-2fa5-4474-962c-1307e5451d30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_sequence_classifier_finbert download started this may take some time.\n",
            "Approximate size to download 390.9 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'negative'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the accuracy of sentiment using Kaggle data (Financial News Headlines)"
      ],
      "metadata": {
        "id": "5GVcdPcr3Ldk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER "
      ],
      "metadata": {
        "id": "-QvbGQDaOR3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Kaggle data to Pandas dataframe and preprocess"
      ],
      "metadata": {
        "id": "QpKHFStDuQJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from Github\n",
        "NER_url = 'https://raw.githubusercontent.com/Brand-Sentiment-Tracking/python-package/main/data/NER_test_data.csv'\n",
        "\n",
        "# Convert data to Pandas dataframe \n",
        "df_NER = pd.read_csv(NER_url).head(500)\n",
        "df_NER.columns = ['Brand', 'text']\n",
        "\n",
        "# Shuffle the DataFrame rows\n",
        "df_NER = df_NER.sample(frac = 1)\n",
        "\n",
        "# Make dataset smaller for faster runtime\n",
        "num_sentences = 100\n",
        "total_num_sentences = df_NER.shape[0]\n",
        "df_NER.drop(df_NER.index[num_sentences:total_num_sentences], inplace=True)\n",
        "\n",
        "df_NER"
      ],
      "metadata": {
        "id": "p7fSpePI1K-G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "eb8b7c68-3493-4b2d-b3a8-890252805e92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-af366330-a633-4f6d-9554-cf6db89c95e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>None</td>\n",
              "      <td>In January-September 2009 , the Group 's net i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>None</td>\n",
              "      <td>Exports grew 16.5 percent to 19.1 million lite...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>Aldata Solution</td>\n",
              "      <td>Finnish Aldata Solution has signed a contract ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>None</td>\n",
              "      <td>Sales for the Department Store Division increa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>None</td>\n",
              "      <td>Commission income increased by 22 % to EUR 4.4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>None</td>\n",
              "      <td>Operating profit totaled EUR 825mn , up from E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>None</td>\n",
              "      <td>The stock rose for a third day on Tuesday brin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>None</td>\n",
              "      <td>Diluted earnings per share ( EPS ) rose to EUR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>Stora Enso</td>\n",
              "      <td>Stora Enso 's third-quarter pre-tax profit dou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>Neste Oil Corp.</td>\n",
              "      <td>Neste Oil Corp. has signed long-term procureme...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af366330-a633-4f6d-9554-cf6db89c95e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af366330-a633-4f6d-9554-cf6db89c95e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af366330-a633-4f6d-9554-cf6db89c95e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "               Brand                                               text\n",
              "212             None  In January-September 2009 , the Group 's net i...\n",
              "329             None  Exports grew 16.5 percent to 19.1 million lite...\n",
              "140  Aldata Solution  Finnish Aldata Solution has signed a contract ...\n",
              "56              None  Sales for the Department Store Division increa...\n",
              "37              None  Commission income increased by 22 % to EUR 4.4...\n",
              "..               ...                                                ...\n",
              "356             None  Operating profit totaled EUR 825mn , up from E...\n",
              "300             None  The stock rose for a third day on Tuesday brin...\n",
              "179             None  Diluted earnings per share ( EPS ) rose to EUR...\n",
              "368       Stora Enso  Stora Enso 's third-quarter pre-tax profit dou...\n",
              "233  Neste Oil Corp.  Neste Oil Corp. has signed long-term procureme...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify the brand in each sentence & compute accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "_NYARSWW3-X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"ner_dl_bert\" # MODEL_NAME = \"onto_100\" / \"ner_dl\"\n",
        "brand_identifier = BrandIdentification(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGW3Pdx9_4q7",
        "outputId": "b0a3549e-5534-47a5-9b6f-de18124d61e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n",
            "ner_dl_bert download started this may take some time.\n",
            "Approximate size to download 15.4 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Measure how long to create a ranked table for one headline (string) only\n",
        "# Randomly select one headline\n",
        "hl_str = df_NER.iloc[8, 1]\n",
        "# print(hl_str)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "df, ranked_df = brand_identifier.create_ranked_table(hl_str)\n",
        "# df.show() \n",
        "# ranked_df.show() # Showing both tables takes 5 seconds\n",
        "\n",
        "mid = time.time()\n",
        "\n",
        "brand = brand_identifier.predict_by_headline(hl_str)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{mid-start} seconds elapsed to create a ranked table for 1 sentence.\")\n",
        "print(f\"{end-mid} seconds elapsed to predict a brand for 1 sentence.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyiTTLWXFw_K",
        "outputId": "d475d1ee-c2eb-417a-84b8-aec24359a510"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.47861266136169434 seconds elapsed to create a ranked table for 1 sentence.\n",
            "4.671874284744263 seconds elapsed to predict a brand for 1 sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Measure how long to create a ranked table for a pandas dataframe of 100 headlines\n",
        "start = time.time()\n",
        "\n",
        "df, ranked_df = brand_identifier.create_ranked_table(df_NER) # The pandas df is changed to a spark df\n",
        "# df.show(300, truncate=False) \n",
        "# ranked_df.show(100, truncate=False) # Showing both tables takes 40 seconds\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{end-start} seconds elapsed to create ranked tables for {num_sentences} sentences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mgGVMmGQuwK",
        "outputId": "a78e600e-b2e4-4748-a231-1a38163bd7e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9220077991485596 seconds elapsed to create ranked tables for 100 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Measure how long to create a ranked table for the list of 100 headlines\n",
        "# Create a list of headline strings\n",
        "hl_list = df_NER['text'].tolist()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "df, ranked_df = brand_identifier.create_ranked_table(hl_list) # The list is first changed into a pandas df, then a spark df\n",
        "# df.show(300, truncate=False) \n",
        "# ranked_df.show(100, truncate=False) # Showing both tables takes 40 seconds\n",
        "\n",
        "end = time.time()\n",
        "print(f\"{end-start} seconds elapsed to create a ranked table for {num_sentences} sentences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqXewWZpDVuO",
        "outputId": "97aec406-8bc3-428a-9c10-18f2ff525e88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8399128913879395 seconds elapsed to create a ranked table for 100 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Measure how long it takes to create a ranked table for each headline, then identify a brand from the table using a for loop\n",
        "start = time.time()\n",
        "\n",
        "# Use list comprehension to identify a brand for each headline (row)\n",
        "df_NER['Predicted Brand'] = [brand_identifier.predict_by_headline(hl) for hl in df_NER['text']]\n",
        "\n",
        "end = time.time()\n",
        "print(f\"{(end-start)} seconds elapsed to classify {num_sentences} sentences.\")\n",
        "\n",
        "# Compute the accuracy\n",
        "y_true = df_NER['Brand'].to_numpy()\n",
        "y_pred = df_NER['Predicted Brand'].to_numpy()\n",
        "print(f\"The accuracy is {100* sum(y_true==y_pred)/len(y_true)}%. \\n\")\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "print(df_NER) # Variation issue in names is rare"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOjUHGMw4DlA",
        "outputId": "96379345-cd68-44ea-c7dd-f729b31fa0e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "718.8277699947357 seconds elapsed to classify 100 sentences.\n",
            "The accuracy is 58.0%. \n",
            "\n",
            "                                Brand  \\\n",
            "212                              None   \n",
            "329                              None   \n",
            "140                   Aldata Solution   \n",
            "56                               None   \n",
            "37                               None   \n",
            "258                              None   \n",
            "235                              None   \n",
            "242                              None   \n",
            "367                              None   \n",
            "236                              None   \n",
            "379                              None   \n",
            "173                              None   \n",
            "175                              None   \n",
            "434                              None   \n",
            "210                              Kone   \n",
            "374                              None   \n",
            "337                              None   \n",
            "84                               None   \n",
            "197                 Beltton-Group Plc   \n",
            "423                              None   \n",
            "159                              None   \n",
            "414                           tinyurl   \n",
            "493                 Latvijas Finieris   \n",
            "54                        Nokia Corp.   \n",
            "330                     Panostaja Oyj   \n",
            "261                              None   \n",
            "383                              None   \n",
            "268                              None   \n",
            "68                               None   \n",
            "15                 Foundries division   \n",
            "413                              None   \n",
            "277                              None   \n",
            "437                       Telko Group   \n",
            "194              Tecnomen Corporation   \n",
            "429                              None   \n",
            "389                              None   \n",
            "196                    Componenta Oyj   \n",
            "50                               None   \n",
            "206                              None   \n",
            "341                              None   \n",
            "363                              None   \n",
            "63                               None   \n",
            "348                              None   \n",
            "135                              None   \n",
            "489                 UPM-Kymmene Corp.   \n",
            "71                               None   \n",
            "239                              None   \n",
            "392  Skandinaviska Enskilda Banken AB   \n",
            "73                         Sponda Plc   \n",
            "154                        Alma Media   \n",
            "105                           Fiskars   \n",
            "38                               None   \n",
            "350                              None   \n",
            "169                              None   \n",
            "438                          Grimaldi   \n",
            "259                              None   \n",
            "65                             Sponda   \n",
            "126                              None   \n",
            "1                             Elcoteq   \n",
            "101                         Sepp+Æl+Æ   \n",
            "301                              Aspo   \n",
            "41                            Nyrstar   \n",
            "263                            Ruukki   \n",
            "422                              None   \n",
            "400                             Tosno   \n",
            "195                       Outotec Oyj   \n",
            "74                               None   \n",
            "272                              None   \n",
            "67                               None   \n",
            "29                           F-Secure   \n",
            "450                              None   \n",
            "100                              None   \n",
            "129                              None   \n",
            "470                 Latvijas Finieris   \n",
            "107                              None   \n",
            "354                              None   \n",
            "484                        Componenta   \n",
            "158      Aviation Systems Maintenance   \n",
            "125                              None   \n",
            "113                              None   \n",
            "398                  Lemminkainen Oyj   \n",
            "208              Svyturys-Utenos Alus   \n",
            "308                             Vacon   \n",
            "472                        Alma Media   \n",
            "246                              None   \n",
            "191                   Fiskars Oyj Abp   \n",
            "243                              None   \n",
            "32                        Viking Line   \n",
            "104                            Biohit   \n",
            "163                              None   \n",
            "457                              None   \n",
            "245                              None   \n",
            "463                              None   \n",
            "244                              None   \n",
            "89                            Fiskars   \n",
            "356                              None   \n",
            "300                              None   \n",
            "179                              None   \n",
            "368                        Stora Enso   \n",
            "233                   Neste Oil Corp.   \n",
            "\n",
            "                                                  text  \\\n",
            "212  In January-September 2009 , the Group 's net i...   \n",
            "329  Exports grew 16.5 percent to 19.1 million lite...   \n",
            "140  Finnish Aldata Solution has signed a contract ...   \n",
            "56   Sales for the Department Store Division increa...   \n",
            "37   Commission income increased by 22 % to EUR 4.4...   \n",
            "258  Order intake grew by 40 % year-on-year and 30 ...   \n",
            "235  Net interest income was EUR 152.2 mn , up from...   \n",
            "242  Operating profit rose to 22.1 mln eur from 19....   \n",
            "367  Sales of security and system packaging increas...   \n",
            "236  Net interest income was EUR 39.3 mn , up from ...   \n",
            "379  The group 's 12-month operating profit grew 31...   \n",
            "173  Both operating profit and turnover for the six...   \n",
            "175                         Cargo volume grew by 7 % .   \n",
            "434  is planning to expand its product portfolio in...   \n",
            "210  In addition , Kone signed a two-year maintenan...   \n",
            "374  The company said that its investments in the n...   \n",
            "337  In addition to the demand in Finland , the exp...   \n",
            "84   No blind-spots coming from 1 vantage point all...   \n",
            "197  Finnish office supplies and computer accessori...   \n",
            "423  Deliveries are to start later in 2010 , and th...   \n",
            "159  Besides we have increased the share of meat in...   \n",
            "414  A tinyurl link takes users to a scamming site ...   \n",
            "493  In August , Latvijas Finieris ordered all prod...   \n",
            "54   Shares of Nokia Corp. rose Thursday after the ...   \n",
            "330  Finnish investment group Panostaja Oyj said it...   \n",
            "261  Passenger volumes rose by 8.4 % in the account...   \n",
            "383  The pretax profit of the group 's life insuran...   \n",
            "268  Previously , the company had guided for EBIT a...   \n",
            "68   The new plant is planned to have an electricit...   \n",
            "15   Foundries division reports its sales increased...   \n",
            "413  A data processing unit collects the data , cal...   \n",
            "277  Sales in Latin America increased by 42 % to EU...   \n",
            "437  KAUKO-TELKO LTD PRESS RELEASE 19.06.2007 AT 14...   \n",
            "194  Finnish messaging solutions developer Tecnomen...   \n",
            "429  German Commerzbank AG 's Hamburg Branch and US...   \n",
            "389  YIT Construction and the town of Riihim+Æki ha...   \n",
            "196  Finnish metal products company Componenta Oyj ...   \n",
            "50   The world 's second largest stainless steel ma...   \n",
            "206  However , sales returned to growth in April-Ju...   \n",
            "341  In the reporting period , the company 's opera...   \n",
            "363  Revenue grew 1 percent to euro742 .2 million U...   \n",
            "63   In June it sold a 30 percent stake to Nordstje...   \n",
            "348  Nevertheless , its market share rose to 49.14 ...   \n",
            "135  `` The priority for 2009 was to strengthen the...   \n",
            "489  Finnish paper maker UPM-Kymmene Corp. on Monda...   \n",
            "71   A maximum of 666,104 new shares can further be...   \n",
            "239  Operating income rose to EUR 696.4 mn from EUR...   \n",
            "392  According to its notice , Skandinaviska Enskil...   \n",
            "73   Finnish real estate investor Sponda Plc said o...   \n",
            "154  Alma Media expects its net sales to increase a...   \n",
            "105  In 2009 , Fiskars ' cash flow from operating a...   \n",
            "38   In January , traffic , measured in revenue pas...   \n",
            "350  October-December sales were 302 mln eur , or a...   \n",
            "169  Both operating profit and net sales for the th...   \n",
            "438  MILAN ( AFX ) - The Italian shipping group Gri...   \n",
            "259  Orders received grew by 55 % year-on-year to E...   \n",
            "65   Sponda Plc Stock Exchange Release 5 December 2...   \n",
            "126  The circuit 's overall production rate on a we...   \n",
            "1    The international electronic industry company ...   \n",
            "101  Clothing chain Sepp+Æl+Æ 's net sales increase...   \n",
            "301  The transaction is in line with Aspo 's strate...   \n",
            "41   Nyrstar has also agreed to supply to Talvivaar...   \n",
            "263  Press release 9 October 2009 Ruukki has signed...   \n",
            "422  Compared with the FTSE 100 index , which rose ...   \n",
            "400  Scandic Feed will also diversify Tosno 's prod...   \n",
            "195  Finnish metal industry solutions supplier Outo...   \n",
            "74   On the route between Helsinki in Finland and T...   \n",
            "272  Profit for the period was EUR 9.8 mn , up from...   \n",
            "67   The contract covers HDO platform , AC800 and C...   \n",
            "29   The Brazilian unit of Finnish security solutio...   \n",
            "450  The company has committed to further expanding...   \n",
            "100  Circulation revenue has increased by 5 % in Fi...   \n",
            "129  The fair value of the company 's investment pr...   \n",
            "470  A In August 2007 , Latvijas Finieris ordered a...   \n",
            "107  In the fourth quarter of 2008 , net sales incr...   \n",
            "354  Operating profit increased to EUR 14.0 mn from...   \n",
            "484  Componenta increased its stake in Turkish stee...   \n",
            "158  Aviation Systems Maintenance is based in Kansa...   \n",
            "125  The bank forecasts Finland 's GDP will grow by...   \n",
            "113  Net sales totaled EUR 93.6 mn , up from EUR 93...   \n",
            "398  Lemminkainen Oyj said that it has signed credi...   \n",
            "208  Svyturys-Utenos Alus , which is controlled by ...   \n",
            "308  Vacon will supply drives to Ruselprom in the p...   \n",
            "472  After the transaction , Alma Media raised its ...   \n",
            "246  Operating profit rose to EUR 26.7 mn from EUR ...   \n",
            "191  Finnish cutlery and hand tools maker Fiskars O...   \n",
            "243  Operating profit rose to EUR 1.6 mn from EUR 1...   \n",
            "32   Viking Line 's cargo revenue increased by 5.4 ...   \n",
            "104  However , Biohit estimates its total net sales...   \n",
            "163  Both operating profit and net sales for the 12...   \n",
            "457  The OMX Nordic 40 ( OMXN40 ) index , comprisin...   \n",
            "245  Operating profit rose to EUR 13.5 mn from EUR ...   \n",
            "463  - Provides summary of the medical equipment pi...   \n",
            "244  Operating profit rose to EUR 103.4 mn from EUR...   \n",
            "89   `` The change will optimize the operational ef...   \n",
            "356  Operating profit totaled EUR 825mn , up from E...   \n",
            "300  The stock rose for a third day on Tuesday brin...   \n",
            "179  Diluted earnings per share ( EPS ) rose to EUR...   \n",
            "368  Stora Enso 's third-quarter pre-tax profit dou...   \n",
            "233  Neste Oil Corp. has signed long-term procureme...   \n",
            "\n",
            "                      Predicted Brand  \n",
            "212                               EUR  \n",
            "329                              None  \n",
            "140                           G.O.L.D  \n",
            "56                            Seppala  \n",
            "37                         Commission  \n",
            "258                               EUR  \n",
            "235                               EUR  \n",
            "242                              None  \n",
            "367                              None  \n",
            "236                               EUR  \n",
            "379                              None  \n",
            "173                              None  \n",
            "175                              None  \n",
            "434       Financial Times Deutschland  \n",
            "210                              Kone  \n",
            "374                              None  \n",
            "337                              None  \n",
            "84                               None  \n",
            "197                 Beltton-Group Plc  \n",
            "423                              None  \n",
            "159                              None  \n",
            "414                            Google  \n",
            "493                 Latvijas Finieris  \n",
            "54                         Nokia Corp  \n",
            "330                     Panostaja Oyj  \n",
            "261                              None  \n",
            "383                              None  \n",
            "268                              EBIT  \n",
            "68                               None  \n",
            "15                       Machine Shop  \n",
            "413                               RVR  \n",
            "277                               EUR  \n",
            "437     KAUKO-TELKO LTD PRESS RELEASE  \n",
            "194                      OMX Helsinki  \n",
            "429                    Hamburg Branch  \n",
            "389                     Travel Centre  \n",
            "196                    Componenta Oyj  \n",
            "50                               None  \n",
            "206                              None  \n",
            "341                              None  \n",
            "363                              None  \n",
            "63                                EQT  \n",
            "348                              None  \n",
            "135                              None  \n",
            "489                        Walki Wisa  \n",
            "71                               None  \n",
            "239                               EUR  \n",
            "392  Skandinaviska Enskilda Banken AB  \n",
            "73                    Danske Bank A-S  \n",
            "154                        Alma Media  \n",
            "105                           Fiskars  \n",
            "38                                RPK  \n",
            "350                              None  \n",
            "169                              None  \n",
            "438                         Finnlines  \n",
            "259                              None  \n",
            "65          Sponda Plc Stock Exchange  \n",
            "126                              None  \n",
            "1                           Postimees  \n",
            "101                               EUR  \n",
            "301                              Aspo  \n",
            "41                            Nyrstar  \n",
            "263                              None  \n",
            "422                              None  \n",
            "400                      Scandic Feed  \n",
            "195                       Outotec Oyj  \n",
            "74                               None  \n",
            "272                              None  \n",
            "67                                HDO  \n",
            "29                       Reseller Web  \n",
            "450                            Apollo  \n",
            "100                              None  \n",
            "129                              None  \n",
            "470                 Latvijas Finieris  \n",
            "107                              None  \n",
            "354                               EUR  \n",
            "484         Doktas Dokumculuk Ticaret  \n",
            "158      Aviation Systems Maintenance  \n",
            "125                              None  \n",
            "113                              None  \n",
            "398                               NDA  \n",
            "208              Svyturys-Utenos Alus  \n",
            "308                         Ruselprom  \n",
            "472                          Talentum  \n",
            "246                               EUR  \n",
            "191                   Fiskars Oyj Abp  \n",
            "243                              None  \n",
            "32                        Viking Line  \n",
            "104                            Biohit  \n",
            "163                              None  \n",
            "457                        OMX Nordic  \n",
            "245                              None  \n",
            "463                              None  \n",
            "244                              None  \n",
            "89                            Fiskars  \n",
            "356                               EUR  \n",
            "300                              None  \n",
            "179                               EUR  \n",
            "368                        Stora Enso  \n",
            "233                             Neste  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify all brands using Spark Dataframe of sentences as input "
      ],
      "metadata": {
        "id": "ehtVDL9_X5lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Try creating spark dataframes in UDF - this doesn't work as no SparkContext should be called inside the function\n",
        "\n",
        "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, MapType, ArrayType\n",
        "\n",
        "# Convert the pandas df to spark df\n",
        "# df_spark_org = spark.createDataFrame(df_NER)  # Only keep the 'text' column\n",
        "# df_spark_org = df_spark_org.limit(10)\n",
        "# df_spark = brand_identifier.pipeline_model.transform(df_spark_org)\n",
        "# df_spark.printSchema()\n",
        "# df_spark.select(\"ner_chunk\").printSchema()\n",
        "\n",
        "# start = time.time()\n",
        "\n",
        "# # Each row of this spark df contains all identified entities for one sentence\n",
        "# df_spark.select(\"ner_chunk\").show(2, truncate=False) \n",
        "# df_spark.select(\"ner_chunk\").first() # pyspark.sql.types.Row\n",
        "\n",
        "# row = df_spark.select(\"ner_chunk\").collect()[1][0]\n",
        "# schema = StructType([StructField(\"annotatorType\", StringType(), True),\n",
        "#                      StructField(\"begin\", IntegerType(), True),\n",
        "#                      StructField(\"end\", IntegerType(), True),\n",
        "#                      StructField(\"result\", StringType(), True), \n",
        "#                      StructField(\"metadata\", MapType(StringType(), StringType()), True),\n",
        "#                      StructField(\"embeddings\", ArrayType(FloatType()), True)])\n",
        "# df = spark.createDataFrame(row, schema = schema)\n",
        "# df = df.select(\n",
        "#     F.col('result').alias('chuck'),\n",
        "#     F.col('metadata.entity').alias('result')\n",
        "# )\n",
        "# df.show()\n",
        "\n",
        "# df.select(F.explode(F.arrays_zip('result',\"metadata\")).alias(\"cols\")).select(\\\n",
        "#             F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "#             F.expr(\"cols['1'].entity\").alias('result'))\n",
        "# rdd = spark.sparkContext.parallelize(row)\n",
        "# df = rdd.toDF()\n",
        "# df.show()\n",
        "\n",
        "# row = df_spark.select(\"ner_chunk\").first()[0][0]\n",
        "# row.result\n",
        "# row.metadata['entity']\n",
        "\n",
        "# pred_brand = udf(lambda z: get_brand(z), StringType()) # Default output is string\n",
        "# spark.udf.register(\"pred_brand\", pred_brand)\n",
        "# def get_brand(row):\n",
        "#     if not row: # If the list is empty, i.e. no ORG identified in headline\n",
        "#         return None \n",
        "\n",
        "#     else:\n",
        "#         # Create a table with entity names and types\n",
        "#         schema = StructType([StructField(\"annotatorType\", StringType(), True),\n",
        "#                  StructField(\"begin\", IntegerType(), True),\n",
        "#                  StructField(\"end\", IntegerType(), True),\n",
        "#                  StructField(\"result\", StringType(), True), \n",
        "#                  StructField(\"metadata\", MapType(StringType(), StringType()), True),\n",
        "#                  StructField(\"embeddings\", ArrayType(FloatType()), True)])\n",
        "#         df = spark.createDataFrame(row, schema = schema)\n",
        "#         df = df.select(F.col('result').alias('chuck'),\n",
        "#                        F.col('metadata.entity').alias('result'))\n",
        "  \n",
        "#         # Filter only ORGs\n",
        "#         df = df.filter(df.result == 'ORG')\n",
        "\n",
        "#         # Rank the ORGs by frequencies\n",
        "#         ranked_df = df.groupBy(df.chunk).count().orderBy('count', ascending=False)\n",
        "        \n",
        "#         # # If no ORG identified in headline, return None\n",
        "#         # if ranked_df.count() == 0:\n",
        "#         #     return None\n",
        "#         # If only one ORG appears in headline, return it\n",
        "#         if ranked_df.count() == 1:\n",
        "#             return ranked_df.first()[0]\n",
        "#         # If one ORG appear more than the others, return that one \n",
        "#         elif ranked_df.first()[1] > ranked_df.collect()[1][1]:\n",
        "#             return ranked_df.first()[0] \n",
        "#         else: # If multiple ORGs appear the same time, return randomly (TO BE MODIFIED)\n",
        "#             return ranked_df.first()[0] \n",
        "\n",
        "# df_spark_combined = df_spark.withColumn('Predicted Brand', pred_brand(\"ner_chunk\"))\n",
        "# df_spark_combined.show(5)\n",
        "\n",
        "# end = time.time()\n",
        "\n",
        "# print(f\"{end-start} seconds elapsed to create ranked tables for {num_sentences} sentences.\")"
      ],
      "metadata": {
        "id": "Yi1cjTqN2Yz-",
        "outputId": "bb8317fe-1d2a-47a6-bc54-a12b835b8f3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(brand='Elcoteq')"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Improve speed of identification using Spark User-defined function\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df_spark_org = spark.createDataFrame(df_NER)  # Only keep the 'text' column\n",
        "# df_spark_org = df_spark_org.limit(10)\n",
        "df_spark = brand_identifier.pipeline_model.transform(df_spark_org)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "pred_brand = udf(lambda z: get_brand(z), StringType()) # Output a string\n",
        "# spark.udf.register(\"pred_brand\", pred_brand)\n",
        "def get_brand(row_list):\n",
        "    if not row_list: # If the list is empty\n",
        "        return \"None\"\n",
        "\n",
        "    else:\n",
        "        # Create a pandas df with entity names and types\n",
        "        data = [[row.result, row.metadata['entity']] for row in row_list]\n",
        "        df_pd = pd.DataFrame(data, columns = ['Entity', 'Type'])\n",
        "  \n",
        "        # Filter only ORGs\n",
        "        df_pd = df_pd[df_pd[\"Type\"] == \"ORG\"]\n",
        "\n",
        "        # Rank the ORGs by frequencies\n",
        "        ranked_df = df_pd[\"Entity\"].value_counts() # a Pandas Series object\n",
        "        \n",
        "        # If no ORG identified in headline, return None\n",
        "        if len(ranked_df.index) == 0:\n",
        "           return \"None\"\n",
        "\n",
        "        # If only one ORG appears in headline, return it\n",
        "        elif len(ranked_df.index) == 1:\n",
        "           return ranked_df.index[0]\n",
        "\n",
        "        # If one ORG appear more than the others, return that one \n",
        "        elif ranked_df[0] > ranked_df[1]:\n",
        "            return ranked_df.index[0] \n",
        "\n",
        "        else: # If multiple ORGs appear the same time, return randomly (TO BE MODIFIED)\n",
        "            return random.choice([ranked_df.index[0], ranked_df.index[1]])\n",
        "\n",
        "# pred_brand_col = pred_brand(df_spark.ner_chunk)\n",
        "df_spark_combined = df_spark.withColumn('Predicted Brand', pred_brand('ner_chunk'))\n",
        "df_spark_final = df_spark_combined.select(\"Brand\", \"Predicted Brand\")\n",
        "df_spark_final.show(100)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{end-start} seconds elapsed to create ranked tables for {num_sentences} sentences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9smD7qJWX5X",
        "outputId": "9bdf8924-f815-444b-8be5-7ecad8ef0013"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               Brand|     Predicted Brand|\n",
            "+--------------------+--------------------+\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|                None|                 EUR|\n",
            "|Helsinki Stock Ex...|           Main List|\n",
            "|      Bank of +_land|Finnish Bank of +...|\n",
            "|             Teleste|              Satlan|\n",
            "|                None|              EUR179|\n",
            "|                None|              Pretax|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|         Viking Line|         Viking Line|\n",
            "|               Tekla|               Tekla|\n",
            "|         TeliaSonera|         TeliaSonera|\n",
            "|                None|                None|\n",
            "|              Ruukki|                None|\n",
            "|       Biohit , Inc.|        Biohit , Inc|\n",
            "|             Comptel|             Comptel|\n",
            "|                None|                None|\n",
            "|Standard Chartere...|    BEIJING XFN-ASIA|\n",
            "|    ESL Shipping Ltd|    ESL Shipping Ltd|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|               Nokia|                None|\n",
            "|                Kone|                Kone|\n",
            "|              Sponda|       SPONDA Sponda|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|      Grimaldi Group|      Grimaldi Group|\n",
            "|                 SEB|                SEK1|\n",
            "|                None|                None|\n",
            "|                None|                 EPS|\n",
            "|                None|                 EUR|\n",
            "|                None|U.S. Patent & Tra...|\n",
            "|               Digia|               Digia|\n",
            "|           Tecnotree|           Tecnotree|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|           Finnlines|           Finnlines|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|       Stonesoft Oyj|       Stonesoft Oyj|\n",
            "|        Marimekko Oy|                 HEL|\n",
            "|                None|                None|\n",
            "|               Nokia|               Nokia|\n",
            "|      KCI Konecranes|                None|\n",
            "|      GeoSentric Oyj|      GeoSentric Oyj|\n",
            "|     Rapala VMC Corp|                 HEL|\n",
            "|                None|                None|\n",
            "|          Konecranes|          Konecranes|\n",
            "|                 YIT|                 YIT|\n",
            "|          Alma Media|          Alma Media|\n",
            "|                None|Department Store ...|\n",
            "|         Vaisala Oyj|            ADP News|\n",
            "|Credit Suisse Fir...|         NORSKE SKOG|\n",
            "|       Outokumpu Oyj|   Outokumpu Oyj HEL|\n",
            "|               Tekla|    Tekla Structures|\n",
            "|               Sappi|Finnish M-real Co...|\n",
            "|                None|                None|\n",
            "|                None|                 EUR|\n",
            "|                None|U.S. Patent & Tra...|\n",
            "|             Pohjola|             Pohjola|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|            Aspocomp|            Aspocomp|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|                None|                None|\n",
            "|            Cargotec|                None|\n",
            "|            Talentum|    Finnish Talentum|\n",
            "|         Outotec Oyj|         Outotec Oyj|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|          Raisio Oyj|               RAIVV|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|       Sports Direct|       Sports Direct|\n",
            "|             Tiimari|             Tiimari|\n",
            "|           Aspo Plc.|           Aspo Plc.|\n",
            "|              Raisio|       Food Division|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|        Nokian Tyres|        Nokian Tyres|\n",
            "|                None|                None|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|                None|                 EUR|\n",
            "|                None|                None|\n",
            "|           Sepp+Æl+Æ|                 EUR|\n",
            "|                None|                None|\n",
            "|   Baltic Pearl CJSC|        Baltic Pearl|\n",
            "|               Bunge|           Sunnuntai|\n",
            "|               Orion|               Orion|\n",
            "|                None|                 EUR|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "23.367811918258667 seconds elapsed to create ranked tables for 100 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the accuracy\n",
        "df_pd_post = df_spark_final.toPandas()\n",
        "\n",
        "y_true = df_pd_post['Brand'].to_numpy()\n",
        "y_pred = df_pd_post['Predicted Brand'].to_numpy()\n",
        "print(f\"The accuracy is {100*sum(y_true==y_pred)/len(y_true)}%. \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBnLS_2bzRZ4",
        "outputId": "a1ac3323-12a8-49aa-8298-259219c23b57"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is 61.0%. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment"
      ],
      "metadata": {
        "id": "LA3mYD2kK2V6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Kaggle data to Pandas dataframe and preprocess"
      ],
      "metadata": {
        "id": "OGdheuC8uMiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_url = 'https://raw.githubusercontent.com/Brand-Sentiment-Tracking/python-package/main/data/sentiment_test_data.csv'\n",
        "\n",
        "# Store data in a Pandas Dataframe\n",
        "df_pandas = pd.read_csv(sentiment_url)\n",
        "\n",
        "# Change column names (pipelines require a \"text\" column to predict)\n",
        "df_pandas.columns = ['True_Sentiment', 'text']\n",
        "\n",
        "# Shuffle the DataFrame rows\n",
        "df_pandas = df_pandas.sample(frac = 1)\n",
        "\n",
        "# Make dataset smaller for faster runtime\n",
        "num_sentences = 100\n",
        "total_num_sentences = df_pandas.shape[0]\n",
        "df_pandas.drop(df_pandas.index[num_sentences:total_num_sentences], inplace=True)\n",
        "\n",
        "print(df_pandas.shape)\n"
      ],
      "metadata": {
        "id": "nwLHDZohOPJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b910475b-4eb2-4c2a-a4a2-8ca5e6f1c65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify the sentiment in each sentence"
      ],
      "metadata": {
        "id": "0jQZPrKIOP3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the identifier object\n",
        "# identifier = SentimentIdentification(MODEL_NAME = \"custom_pipeline\") # 90.2% accuracy on 500 sentences 89.8% on 1000 sentences\n",
        "identifier = SentimentIdentification(MODEL_NAME =  \"classifierdl_bertwiki_finance_sentiment_pipeline\") # Alternative pretrained pipeline 90.0% accuracy on 500 sentences\n",
        "\n",
        "preds = []\n",
        "target = []\n",
        "ignored_idxs = []\n",
        "sentiment_to_ignore = \"\" # e.g. neutral\n",
        "\n",
        "# Measure how long it takes\n",
        "start = time.time()\n",
        "\n",
        "# Collect predicted sentiment for each headline - take three minutes to run\n",
        "for idx, hl in enumerate(df_pandas['text']):\n",
        "\n",
        "    # Only append the sentiment if it is not the sentiment to ignore (e.g. neutral)\n",
        "    target_sentiment = df_pandas[\"True_Sentiment\"][df_pandas.index[idx]]\n",
        "\n",
        "    if target_sentiment != sentiment_to_ignore:\n",
        "      preds.append(identifier.predict(hl))\n",
        "    else:\n",
        "      ignored_idxs.append(idx)\n",
        "\n",
        "    # Print progress\n",
        "    if idx % 25 == 0:\n",
        "      print(f\"Classification {100*idx/num_sentences}% done.\")\n",
        "\n",
        "# Remove all ignored entries from dataset\n",
        "df_pandas.drop(df_pandas.index[ignored_idxs], inplace=True)\n",
        "\n",
        "df_pandas['Predicted_Sentiment'] = preds\n",
        "\n",
        "# Measure how long it takes\n",
        "end = time.time()\n",
        "print(f\"{end-start} seconds elapsed to classify {num_sentences} sentences.\")\n",
        "\n",
        "# Modify predicted labels to match with true labels\n",
        "# df = df.replace({'Predicted Sentiment': {'pos' : 'positive', 'neg' : 'negative'}})\n",
        "\n",
        "df_pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "5jFhoWw54zMo",
        "outputId": "f12bd1e4-d316-421f-df10-5b0106514f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifierdl_bertwiki_finance_sentiment_pipeline download started this may take some time.\n",
            "Approx size to download 412.9 MB\n",
            "[OK!]\n",
            "Classification 0.0% done.\n",
            "Classification 25.0% done.\n",
            "Classification 50.0% done.\n",
            "Classification 75.0% done.\n",
            "33.950716972351074 seconds elapsed to classify 100 sentences.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6be4f4f-b512-454e-b43a-e647d8c5a05f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True_Sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>Predicted_Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3667</th>\n",
              "      <td>neutral</td>\n",
              "      <td>ADP News - Jan 13 , 2009 - Finnish industrial ...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2009</th>\n",
              "      <td>positive</td>\n",
              "      <td>Elcoteq 's stock of orders has stabilised in t...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1177</th>\n",
              "      <td>neutral</td>\n",
              "      <td>As a result of the merger , the largest profes...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2771</th>\n",
              "      <td>neutral</td>\n",
              "      <td>AffectoGenimap builds highly customised IT sol...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3903</th>\n",
              "      <td>neutral</td>\n",
              "      <td>The solution is demonstrated on a tablet devel...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>positive</td>\n",
              "      <td>The cooperation will double The Switch 's conv...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3439</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Union and company officials did not return cal...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>positive</td>\n",
              "      <td>Via the Satlan acquisition , Teleste plans to ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>875</th>\n",
              "      <td>positive</td>\n",
              "      <td>Profitability ( EBIT % ) was 13.9 % , compared...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Rautakesko 's business operations in Norway an...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6be4f4f-b512-454e-b43a-e647d8c5a05f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6be4f4f-b512-454e-b43a-e647d8c5a05f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6be4f4f-b512-454e-b43a-e647d8c5a05f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     True_Sentiment  ... Predicted_Sentiment\n",
              "3667        neutral  ...             neutral\n",
              "2009       positive  ...            positive\n",
              "1177        neutral  ...             neutral\n",
              "2771        neutral  ...             neutral\n",
              "3903        neutral  ...             neutral\n",
              "...             ...  ...                 ...\n",
              "595        positive  ...            positive\n",
              "3439        neutral  ...             neutral\n",
              "310        positive  ...            positive\n",
              "875        positive  ...            positive\n",
              "539         neutral  ...            positive\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Measure the Accuracy"
      ],
      "metadata": {
        "id": "OugF9Z-6t0PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = df_pandas['True_Sentiment'].to_numpy()\n",
        "y_pred = df_pandas['Predicted_Sentiment'].to_numpy()\n",
        "\n",
        "print(f\"The accuracy is {100* sum(y_true==y_pred)/len(y_true)}%. \\n\")\n",
        "\n",
        "target_names = ['positive', 'neutral', 'negative']\n",
        "\n",
        "# Compute classification metrics - poor accuracy\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiLV0Glo_Kzj",
        "outputId": "d4a850e6-af03-4d40-a099-177b6e8ba2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is 90.0%. \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       1.00      0.93      0.97        15\n",
            "     neutral       0.90      0.92      0.91        50\n",
            "    negative       0.86      0.86      0.86        35\n",
            "\n",
            "    accuracy                           0.90       100\n",
            "   macro avg       0.92      0.90      0.91       100\n",
            "weighted avg       0.90      0.90      0.90       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classify using Spark Dataframe as input"
      ],
      "metadata": {
        "id": "PJc83LMsgJqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_join\n",
        "from pyspark.sql.functions import col, explode, expr, greatest\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
        "\n",
        "# Define pretrained pipeline\n",
        "pipeline = PretrainedPipeline(\"classifierdl_bertwiki_finance_sentiment_pipeline\", lang = 'en')\n",
        "# identifier = SentimentIdentification(MODEL_NAME = \"custom_pipeline\")\n",
        "\n",
        "# Convert to spark dataframe for faster prediction\n",
        "df_spark = spark.createDataFrame(df_pandas) \n",
        "\n",
        "# Measure how long it takes\n",
        "start = time.time()\n",
        "\n",
        "# Predict the sentiment\n",
        "df_spark = pipeline.transform(df_spark)\n",
        "\n",
        "# # df_spark = identifier.pipeline_model.transform(df_spark)\n",
        "\n",
        "\n",
        "# print(df_spark.first()['class'])\n",
        "# df_spark.printSchema()\n",
        "\n",
        "#Extract sentiment score\n",
        "df_spark_scores = df_spark.select(explode(col(\"class.metadata\")).alias(\"metadata\")).select(col(\"metadata\")[\"positive\"].alias(\"positive\"),\n",
        "                                                                                    col(\"metadata\")[\"neutral\"].alias(\"neutral\"),\n",
        "                                                                                    col(\"metadata\")[\"negative\"].alias(\"negative\"))\n",
        "\n",
        "# df_spark_scores = df_spark_scores.withColumn('max_val', greatest('positive', 'negative', 'neutral')) # Doesn't work because of scientific notation\n",
        "\n",
        "# Extract only targets and labels\n",
        "df_spark = df_spark.select(\"text\", \"True_Sentiment\", \"class.result\")\n",
        "\n",
        "\n",
        "# # df_spark_no_text = df_spark.select(\"True_Sentiment\", \"result\")\n",
        "# # df_spark_no_text = df_spark_no_text.withColumn(\"Predicted_Sentiment\", array_join(\"result\", \"\"))\n",
        "\n",
        "# Rename to Predicted Sentiment\n",
        "df_spark = df_spark.withColumnRenamed(\"result\",\"Predicted_Sentiment\")\n",
        "\n",
        "# Convert sentiment from a list to a string\n",
        "df_spark = df_spark.withColumn(\"Predicted_Sentiment\", array_join(\"Predicted_Sentiment\", \"\"))\n",
        "\n",
        "# Merge the predictions and the confidence scores\n",
        "\n",
        "# Add temporary column index to join\n",
        "w = Window.orderBy(monotonically_increasing_id())\n",
        "df_spark_with_index =  df_spark.withColumn(\"columnindex\", row_number().over(w))\n",
        "df_spark_scores_with_index =  df_spark_scores.withColumn(\"columnindex\", row_number().over(w))\n",
        "\n",
        "# Join the predictions and the scores in one dataframe\n",
        "df_spark_with_index = df_spark_with_index.join(df_spark_scores_with_index,\n",
        "                         df_spark_with_index.columnindex == df_spark_scores_with_index.columnindex,\n",
        "                         'inner').drop(df_spark_scores_with_index.columnindex)\n",
        "\n",
        "# Remove the index column\n",
        "df_spark_combined = df_spark_with_index.drop(df_spark_with_index.columnindex)\n",
        "\n",
        "# Convert to pandas dataframe for postprocessing (https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32)\n",
        "df_pandas_postprocessed = df_spark_combined.toPandas()\n",
        "# df_pandas_postprocessed = df_spark.toPandas()\n",
        "\n",
        "# df_pandas[\"Predicted_Sentiment\"] = df_pandas[\"Predicted_Sentiment\"].apply(lambda x: x[0]) # Alternative to convert list to string\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "print(f\"{end-start} seconds elapsed to classify {num_sentences} sentences.\")\n",
        "\n",
        "# df_pandas_post_processed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plG0dY_MZu5Z",
        "outputId": "0a152cfa-6bc3-4b8c-f047-3f30e62f001e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifierdl_bertwiki_finance_sentiment_pipeline download started this may take some time.\n",
            "Approx size to download 412.9 MB\n",
            "[OK!]\n",
            "26.504663228988647 seconds elapsed to classify 100 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the Accuracy"
      ],
      "metadata": {
        "id": "jsCyAYiYOutA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy = accuracy_score(df_pandas[\"True_Sentiment\"], df_pandas[\"Predicted_Sentiment\"])\n",
        "print(f\"The accuracy is {accuracy*100}%.\")\n",
        "print(classification_report(df_pandas[\"True_Sentiment\"], df_pandas[\"Predicted_Sentiment\"]))\n",
        "\n",
        "# Alternatively if not converted to pandas dataframe, use the following for the accuracy\n",
        "# Compute accuracy by comparing each true label with predicted label\n",
        "start = time.time()\n",
        "accuracy = df_spark.filter(df_spark.Predicted_Sentiment == df_spark.True_Sentiment).count()/ num_sentences\n",
        "end = time.time()\n",
        "print(f\"{end-start} seconds elapsed to calculate accuracy of {num_sentences} sentences.\")\n",
        "print(f\"The accuracy is {accuracy*100}%.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZsstjzhOtko",
        "outputId": "640c8de5-2bfa-4186-a89d-5c97415047c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is 90.0%.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.93      0.97        15\n",
            "     neutral       0.90      0.92      0.91        50\n",
            "    positive       0.86      0.86      0.86        35\n",
            "\n",
            "    accuracy                           0.90       100\n",
            "   macro avg       0.92      0.90      0.91       100\n",
            "weighted avg       0.90      0.90      0.90       100\n",
            "\n",
            "28.48450493812561 seconds elapsed to calculate accuracy of 100 sentences.\n",
            "The accuracy is 90.0%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternatively extract predictions as strings (takes much longer)"
      ],
      "metadata": {
        "id": "bgCjIm4CK8mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract the predictions from the dataframe\n",
        "# annotations_list = result.select(\"class.result\").collect()\n",
        "# sentiment_list = [annotations_list[i].result[0] for i in range(num_sentences)]\n",
        "\n",
        "# # Annotate previous dataframe for visualization\n",
        "# df_pandas['Predicted Sentiment'] = sentiment_list\n",
        "\n",
        "# # Move text column to the beginning\n",
        "# text_column = df_pandas.pop('text')\n",
        "# df_pandas.insert(0, 'Headline', text_column)\n",
        "\n",
        "# display(df_pandas)\n",
        "\n",
        "# y_true = df_pandas['True Sentiment'].to_numpy()\n",
        "# y_pred = df_pandas['Predicted Sentiment'].to_numpy()\n",
        "\n",
        "# print(f\"The accuracy is {100* sum(y_true==y_pred)/len(y_true)}%. \\n\")"
      ],
      "metadata": {
        "id": "FE7qhsf8LAre"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BDijGosdPGzW",
        "rmq_-RXkbB1d",
        "-5m65UosNdY5",
        "MwBY37mRbKjm",
        "HoTrh-sEUeRf",
        "QpKHFStDuQJ-",
        "OugF9Z-6t0PT"
      ],
      "name": "Charlize - CLASS FOR SENTIMENT DETECTION USING SNOW LABS PIPELINES.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}